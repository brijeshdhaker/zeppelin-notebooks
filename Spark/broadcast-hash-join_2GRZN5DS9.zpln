{
  "paragraphs": [
    {
      "text": "%spark.conf\n\nSPARK_HOME\u003d/opt/spark\n\nmaster yarn\n\nspark.submit.deployMode client\n\nspark.driver.memory 1G\n",
      "user": "anonymous",
      "dateUpdated": "2022-10-21 02:03:31.795",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "text",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/text",
        "fontSize": 12.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1640887371473_1073052224",
      "id": "paragraph_1640887371473_1073052224",
      "dateCreated": "2021-12-30 18:02:51.474",
      "dateStarted": "2022-10-21 02:03:31.800",
      "dateFinished": "2022-10-21 02:03:31.813",
      "status": "FINISHED"
    },
    {
      "title": "Disable Brodcast Join",
      "text": "%spark\n\nspark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\",-1)",
      "user": "anonymous",
      "dateUpdated": "2022-10-21 02:03:37.888",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "fontSize": 12.0,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1640887160384_472269493",
      "id": "paragraph_1640887160384_472269493",
      "dateCreated": "2021-12-30 17:59:20.385",
      "dateStarted": "2022-10-21 02:03:37.891",
      "dateFinished": "2022-10-21 02:04:14.092",
      "status": "FINISHED"
    },
    {
      "title": "Join Performance without Broadcast",
      "text": "%spark\n\n// Loading github.json into a dataframe\nval githubDF \u003d spark.read.json(\"/datasets/github.json\")\n//githubDF.printSchema\n//githubDF.show()\n\n\n// Loading github-top20.json into a dataframe\nval githubTop20DF \u003d spark.read.json(\"/datasets/github-top20.json\")\ngithubTop20DF.printSchema\ngithubTop20DF.show()\n\n\n// Or you can specify the join condition explicitly in case the key is different between tables\nval topContributorsJoinedDF \u003d githubDF.join(githubTop20DF, githubDF(\"actor.login\") \u003d\u003d\u003d githubTop20DF(\"login\"))\n\nimport spark.implicits._\n\n// Counting the results, and ordering by descending count.\ntopContributorsJoinedDF.groupBy(\"actor.login\").count().orderBy(col(\"count\").desc).show()\n//topContributorsJoinedDF.groupBy(\"actor.login\").count.orderBy(\u0027count.desc).show()\n\n\n",
      "user": "anonymous",
      "dateUpdated": "2022-10-21 02:04:18.574",
      "progress": 100,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "fontSize": 12.0,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "root\n |-- login: string (nullable \u003d true)\n\n+------------------+\n|             login|\n+------------------+\n|GoogleCodeExporter|\n|         stackmutt|\n|      greatfirebot|\n|diversify-exp-user|\n|            kwurst|\n|   direwolf-github|\n|     KenanSulayman|\n|        jack-oquin|\n|        manuelrp07|\n|    mirror-updates|\n|     tryton-mirror|\n|        EstifanosG|\n|           houndci|\n|      jeff1evesque|\n|      LukasReschke|\n|       nwt-patrick|\n|           Somasis|\n|        mikegazdag|\n|       tterrag1098|\n|   EmanueleMinotto|\n+------------------+\n\n+------------------+-----+\n|             login|count|\n+------------------+-----+\n|GoogleCodeExporter| 2073|\n|         stackmutt|  284|\n|      greatfirebot|  192|\n|diversify-exp-user|  146|\n|            kwurst|   92|\n|   direwolf-github|   88|\n|     KenanSulayman|   72|\n|        jack-oquin|   52|\n|        manuelrp07|   45|\n|    mirror-updates|   42|\n|     tryton-mirror|   37|\n|        EstifanosG|   32|\n|           houndci|   30|\n|      jeff1evesque|   29|\n|      LukasReschke|   28|\n|       nwt-patrick|   27|\n|           Somasis|   27|\n|        mikegazdag|   26|\n|       tterrag1098|   23|\n|   EmanueleMinotto|   22|\n+------------------+-----+\n\n\u001b[1m\u001b[34mgithubDF\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m \u003d [actor: struct\u003cavatar_url: string, gravatar_id: string ... 3 more fields\u003e, created_at: string ... 6 more fields]\n\u001b[1m\u001b[34mgithubTop20DF\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m \u003d [login: string]\n\u001b[1m\u001b[34mtopContributorsJoinedDF\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m \u003d [actor: struct\u003cavatar_url: string, gravatar_id: string ... 3 more fields\u003e, created_at: string ... 7 more fields]\nimport spark.implicits._\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://zeppelin:4040/jobs/job?id\u003d0"
            },
            {
              "jobUrl": "http://zeppelin:4040/jobs/job?id\u003d1"
            },
            {
              "jobUrl": "http://zeppelin:4040/jobs/job?id\u003d2"
            },
            {
              "jobUrl": "http://zeppelin:4040/jobs/job?id\u003d3"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1640887822291_2091545505",
      "id": "paragraph_1640887822291_2091545505",
      "dateCreated": "2021-12-30 18:10:22.291",
      "dateStarted": "2022-10-21 02:04:18.577",
      "dateFinished": "2022-10-21 02:04:30.187",
      "status": "FINISHED"
    },
    {
      "text": "%spark\n\ntopContributorsJoinedDF.explain()",
      "user": "anonymous",
      "dateUpdated": "2022-10-21 02:04:46.301",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "fontSize": 12.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u003d\u003d Physical Plan \u003d\u003d\nSortMergeJoin [actor#7.login], [login#30], Inner\n:- Sort [actor#7.login ASC NULLS FIRST], false, 0\n:  +- Exchange hashpartitioning(actor#7.login, 200), ENSURE_REQUIREMENTS, [id\u003d#104]\n:     +- Filter isnotnull(actor#7)\n:        +- FileScan json [actor#7,created_at#8,id#9,org#10,payload#11,public#12,repo#13,type#14] Batched: false, DataFilters: [isnotnull(actor#7)], Format: JSON, Location: InMemoryFileIndex[hdfs://namenode:9000/datasets/github.json], PartitionFilters: [], PushedFilters: [IsNotNull(actor)], ReadSchema: struct\u003cactor:struct\u003cavatar_url:string,gravatar_id:string,id:bigint,login:string,url:string\u003e,creat...\n+- *(2) Sort [login#30 ASC NULLS FIRST], false, 0\n   +- Exchange hashpartitioning(login#30, 200), ENSURE_REQUIREMENTS, [id\u003d#112]\n      +- *(1) Filter isnotnull(login#30)\n         +- FileScan json [login#30] Batched: false, DataFilters: [isnotnull(login#30)], Format: JSON, Location: InMemoryFileIndex[hdfs://namenode:9000/datasets/github-top20.json], PartitionFilters: [], PushedFilters: [IsNotNull(login)], ReadSchema: struct\u003clogin:string\u003e\n\n\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1640914777382_654592376",
      "id": "paragraph_1640914777382_654592376",
      "dateCreated": "2021-12-31 01:39:37.382",
      "dateStarted": "2022-10-21 02:04:46.305",
      "dateFinished": "2022-10-21 02:04:46.479",
      "status": "FINISHED"
    },
    {
      "title": "Enable Brodcast Join",
      "text": "%spark\n\n//Renable automatic broadcasting\nspark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\",1024*1024*10)\n\n// Loading github.json into a dataframe\nval githubDF \u003d spark.read.json(\"/datasets/github.json\")\n//githubDF.printSchema\n//githubDF.show()\n\n\n// Loading github-top20.json into a dataframe\nval githubTop20DF \u003d spark.read.json(\"/datasets/github-top20.json\")\n//githubTop20DF.printSchema\n//githubTop20DF.show()\n\n\n// Or you can specify the join condition explicitly in case the key is different between tables\nval topContributorsJoinedDF \u003d githubDF.join(githubTop20DF, githubDF(\"actor.login\") \u003d\u003d\u003d githubTop20DF(\"login\"))\n\nimport spark.implicits._\n\n// Counting the results, and ordering by descending count.\ntopContributorsJoinedDF.groupBy(\"actor.login\").count().orderBy(col(\"count\").desc).show()\n//topContributorsJoinedDF.groupBy(\"actor.login\").count.orderBy(\u0027count.desc).show()\n",
      "user": "anonymous",
      "dateUpdated": "2022-10-21 02:05:58.688",
      "progress": 100,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "fontSize": 12.0,
        "results": {},
        "enabled": true,
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+------------------+-----+\n|             login|count|\n+------------------+-----+\n|GoogleCodeExporter| 2073|\n|         stackmutt|  284|\n|      greatfirebot|  192|\n|diversify-exp-user|  146|\n|            kwurst|   92|\n|   direwolf-github|   88|\n|     KenanSulayman|   72|\n|        jack-oquin|   52|\n|        manuelrp07|   45|\n|    mirror-updates|   42|\n|     tryton-mirror|   37|\n|        EstifanosG|   32|\n|           houndci|   30|\n|      jeff1evesque|   29|\n|      LukasReschke|   28|\n|       nwt-patrick|   27|\n|           Somasis|   27|\n|        mikegazdag|   26|\n|       tterrag1098|   23|\n|   EmanueleMinotto|   22|\n+------------------+-----+\n\n\u001b[1m\u001b[34mgithubDF\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m \u003d [actor: struct\u003cavatar_url: string, gravatar_id: string ... 3 more fields\u003e, created_at: string ... 6 more fields]\n\u001b[1m\u001b[34mgithubTop20DF\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m \u003d [login: string]\n\u001b[1m\u001b[34mtopContributorsJoinedDF\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m \u003d [actor: struct\u003cavatar_url: string, gravatar_id: string ... 3 more fields\u003e, created_at: string ... 7 more fields]\nimport spark.implicits._\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://zeppelin:4040/jobs/job?id\u003d4"
            },
            {
              "jobUrl": "http://zeppelin:4040/jobs/job?id\u003d5"
            },
            {
              "jobUrl": "http://zeppelin:4040/jobs/job?id\u003d7"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1640890399555_1395854605",
      "id": "paragraph_1640890399555_1395854605",
      "dateCreated": "2021-12-30 18:53:19.555",
      "dateStarted": "2022-10-21 02:05:58.693",
      "dateFinished": "2022-10-21 02:06:01.916",
      "status": "FINISHED"
    },
    {
      "text": "%spark\n\ntopContributorsJoinedDF.explain()",
      "user": "anonymous",
      "dateUpdated": "2022-10-21 02:07:11.117",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "fontSize": 12.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u003d\u003d Physical Plan \u003d\u003d\nBroadcastHashJoin [actor#93.login], [login#116], Inner, BuildRight, false\n:- Filter isnotnull(actor#93)\n:  +- FileScan json [actor#93,created_at#94,id#95,org#96,payload#97,public#98,repo#99,type#100] Batched: false, DataFilters: [isnotnull(actor#93)], Format: JSON, Location: InMemoryFileIndex[hdfs://namenode:9000/datasets/github.json], PartitionFilters: [], PushedFilters: [IsNotNull(actor)], ReadSchema: struct\u003cactor:struct\u003cavatar_url:string,gravatar_id:string,id:bigint,login:string,url:string\u003e,creat...\n+- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [id\u003d#211]\n   +- *(1) Filter isnotnull(login#116)\n      +- FileScan json [login#116] Batched: false, DataFilters: [isnotnull(login#116)], Format: JSON, Location: InMemoryFileIndex[hdfs://namenode:9000/datasets/github-top20.json], PartitionFilters: [], PushedFilters: [IsNotNull(login)], ReadSchema: struct\u003clogin:string\u003e\n\n\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1641201327174_615923441",
      "id": "paragraph_1641201327174_615923441",
      "dateCreated": "2022-01-03 09:15:27.175",
      "dateStarted": "2022-10-21 02:07:11.120",
      "dateFinished": "2022-10-21 02:07:11.263",
      "status": "FINISHED"
    },
    {
      "title": "Using Broadcast Join Hint",
      "text": "%pyspark\n\nfrom pyspark.sql.functions import *\n\n# Disable Broadcast Join\nspark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\",-1)\nspark.conf.set(\"spark.sql.shuffle.partitions\", 3)\n\n\ndf1 \u003d spark.read.json(\"/datasets/github.json\")\ndf2 \u003d spark.read.json(\"/datasets/github-top20.json\")\n\njoinExper \u003d df1[\"actor.login\"] \u003d\u003d df2[\"login\"]\njoinDF \u003d df1.join(broadcast(df2), joinExper, \"inner\")\n\n#joinDF.show()\n\n# Foreach example\n# def f(x): print(x)\n# joinDF.foreach(f)\n\n# Another example\njoinDF.count()\n\n",
      "user": "anonymous",
      "dateUpdated": "2022-10-21 02:07:35.549",
      "progress": 100,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "fontSize": 12.0,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "org.apache.zeppelin.interpreter.InterpreterException: org.apache.zeppelin.interpreter.InterpreterException: Fail to open PythonInterpreter\n\tat org.apache.zeppelin.interpreter.LazyOpenInterpreter.open(LazyOpenInterpreter.java:76)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:844)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:752)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:172)\n\tat org.apache.zeppelin.scheduler.AbstractScheduler.runJob(AbstractScheduler.java:132)\n\tat org.apache.zeppelin.scheduler.FIFOScheduler.lambda$runJobInScheduler$0(FIFOScheduler.java:42)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.zeppelin.interpreter.InterpreterException: Fail to open PythonInterpreter\n\tat org.apache.zeppelin.python.PythonInterpreter.open(PythonInterpreter.java:123)\n\tat org.apache.zeppelin.spark.PySparkInterpreter.open(PySparkInterpreter.java:94)\n\tat org.apache.zeppelin.interpreter.LazyOpenInterpreter.open(LazyOpenInterpreter.java:70)\n\t... 8 more\nCaused by: java.io.IOException: Fail to launch python process.\norg.apache.commons.exec.ExecuteException: Execution failed (Exit value: -559038737. Caused by java.io.IOException: Cannot run program \"./environment/bin/python\" (in directory \".\"): error\u003d2, No such file or directory)\n\tat org.apache.commons.exec.DefaultExecutor$1.run(DefaultExecutor.java:205)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.io.IOException: Cannot run program \"./environment/bin/python\" (in directory \".\"): error\u003d2, No such file or directory\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)\n\tat java.lang.Runtime.exec(Runtime.java:621)\n\tat org.apache.commons.exec.launcher.Java13CommandLauncher.exec(Java13CommandLauncher.java:61)\n\tat org.apache.commons.exec.DefaultExecutor.launch(DefaultExecutor.java:279)\n\tat org.apache.commons.exec.DefaultExecutor.executeInternal(DefaultExecutor.java:336)\n\tat org.apache.commons.exec.DefaultExecutor.access$200(DefaultExecutor.java:48)\n\tat org.apache.commons.exec.DefaultExecutor$1.run(DefaultExecutor.java:200)\n\t... 1 more\nCaused by: java.io.IOException: error\u003d2, No such file or directory\n\tat java.lang.UNIXProcess.forkAndExec(Native Method)\n\tat java.lang.UNIXProcess.\u003cinit\u003e(UNIXProcess.java:247)\n\tat java.lang.ProcessImpl.start(ProcessImpl.java:134)\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)\n\t... 7 more\n\n\tat org.apache.zeppelin.python.PythonInterpreter.createGatewayServerAndStartScript(PythonInterpreter.java:170)\n\tat org.apache.zeppelin.python.PythonInterpreter.open(PythonInterpreter.java:120)\n\t... 10 more\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1641184938968_1677982082",
      "id": "paragraph_1641184938968_1677982082",
      "dateCreated": "2022-01-03 04:42:18.968",
      "dateStarted": "2022-10-21 02:07:35.554",
      "dateFinished": "2022-10-21 02:07:35.713",
      "status": "ERROR"
    },
    {
      "text": "%pyspark\n\njoinDF.explain()\n\n",
      "user": "anonymous",
      "dateUpdated": "2022-02-02 05:02:38.801",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "fontSize": 12.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u003d\u003d Physical Plan \u003d\u003d\nBroadcastHashJoin [actor#1398.login], [login#1420], Inner, BuildRight\n:- Project [actor#1398, created_at#1399, id#1400, org#1401, payload#1402, public#1403, repo#1404, type#1405]\n:  +- Filter isnotnull(actor#1398)\n:     +- FileScan json [actor#1398,created_at#1399,id#1400,org#1401,payload#1402,public#1403,repo#1404,type#1405] Batched: false, Format: JSON, Location: InMemoryFileIndex[hdfs://namenode:9000/datasets/github.json], PartitionFilters: [], PushedFilters: [IsNotNull(actor)], ReadSchema: struct\u003cactor:struct\u003cavatar_url:string,gravatar_id:string,id:bigint,login:string,url:string\u003e,creat...\n+- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]))\n   +- *(1) Project [login#1420]\n      +- *(1) Filter isnotnull(login#1420)\n         +- *(1) FileScan json [login#1420] Batched: false, Format: JSON, Location: InMemoryFileIndex[hdfs://namenode:9000/datasets/github-top20.json], PartitionFilters: [], PushedFilters: [IsNotNull(login)], ReadSchema: struct\u003clogin:string\u003e\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1640890439987_243762451",
      "id": "paragraph_1640890439987_243762451",
      "dateCreated": "2021-12-30 18:53:59.987",
      "dateStarted": "2022-01-03 07:24:13.235",
      "dateFinished": "2022-01-03 07:24:13.456",
      "status": "FINISHED"
    },
    {
      "text": "%spark\n\n\n",
      "user": "anonymous",
      "dateUpdated": "2022-02-02 05:02:46.535",
      "progress": 0,
      "config": {
        "colWidth": 12.0,
        "fontSize": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1640914842278_627600917",
      "id": "paragraph_1640914842278_627600917",
      "dateCreated": "2021-12-31 01:40:42.279",
      "status": "READY"
    }
  ],
  "name": "broadcast-hash-join",
  "id": "2GRZN5DS9",
  "defaultInterpreterGroup": "spark",
  "version": "0.10.0",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {},
  "config": {
    "isZeppelinNotebookCronEnable": false
  },
  "info": {}
}