{
  "paragraphs": [
    {
      "text": "%md\n\n### Which storage level to use?\n\n#### `MEMORY_ONLY` : This option is CPU efficient and optimal for performance\n\n#### `MEMORY_ONLY_SER` : This option if the RDD, DataFrame or Dataset has a lot of elements. Use an efficient serialization framework like Kyro. This option is space efficient.\n\nSpilling data to disk is expensive so use DISK_ONLY and options like MEMORY_AND_DISK only when the computation involved in getting to the RDD, DataFrame or Dataset that is being persisted is expensive.\n\nUse replication options like MEMORY_ONLY_2 only you have a mandate for full fast recovery.\n\n### When to use cache and persist functions in Spark ?\n* Performing multiple actions on the same RDD\n* Iterative transformations\n* Debug memory or other data issues\n\n\n\n",
      "user": "anonymous",
      "dateUpdated": "2021-12-31 10:46:29.845",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "fontSize": 12.0,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003eWhich storage level to use?\u003c/h3\u003e\n\u003ch4\u003e\u003ccode\u003eMEMORY_ONLY\u003c/code\u003e : This option is CPU efficient and optimal for performance\u003c/h4\u003e\n\u003ch4\u003e\u003ccode\u003eMEMORY_ONLY_SER\u003c/code\u003e : This option if the RDD, DataFrame or Dataset has a lot of elements. Use an efficient serialization framework like Kyro. This option is space efficient.\u003c/h4\u003e\n\u003cp\u003eSpilling data to disk is expensive so use DISK_ONLY and options like MEMORY_AND_DISK only when the computation involved in getting to the RDD, DataFrame or Dataset that is being persisted is expensive.\u003c/p\u003e\n\u003cp\u003eUse replication options like MEMORY_ONLY_2 only you have a mandate for full fast recovery.\u003c/p\u003e\n\u003ch3\u003eWhen to use cache and persist functions in Spark ?\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ePerforming multiple actions on the same RDD\u003c/li\u003e\n\u003cli\u003eIterative transformations\u003c/li\u003e\n\u003cli\u003eDebug memory or other data issues\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1640928686019_1833920782",
      "id": "paragraph_1640928686019_1833920782",
      "dateCreated": "2021-12-31 05:31:26.019",
      "dateStarted": "2021-12-31 10:46:29.846",
      "dateFinished": "2021-12-31 10:46:29.860",
      "status": "FINISHED"
    },
    {
      "text": "%spark.conf\n\nSPARK_HOME\u003d/opt/spark-3.1.2\nmaster yarn\nspark.submit.deployMode client\nspark.driver.memory 1G\n",
      "user": "anonymous",
      "dateUpdated": "2022-01-10 16:52:13.654",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "text",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/text",
        "fontSize": 10.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1640927067114_209650297",
      "id": "paragraph_1640887371473_1073052224",
      "dateCreated": "2021-12-31 05:04:27.114",
      "dateStarted": "2021-12-31 05:27:30.407",
      "dateFinished": "2021-12-31 05:27:30.418",
      "status": "FINISHED"
    },
    {
      "title": "Read Data Frame",
      "text": "%spark\n\n// Loading github.json into a dataframe\nval githubDF \u003d spark.read.json(\"hdfs://namenode:9000/datasets/github.json\")\n//githubDF.printSchema\n//githubDF.show()\n\n\n",
      "user": "anonymous",
      "dateUpdated": "2023-02-15 02:53:14.558",
      "progress": 100,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "fontSize": 10.0,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mgithubDF\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m \u003d [actor: struct\u003cavatar_url: string, gravatar_id: string ... 3 more fields\u003e, created_at: string ... 6 more fields]\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://resourcemanager:8088/proxy/application_1676427769595_0004//jobs/job?id\u003d4"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1640927067115_566163187",
      "id": "paragraph_1640887822291_2091545505",
      "dateCreated": "2021-12-31 05:04:27.115",
      "dateStarted": "2023-02-15 02:53:14.562",
      "dateFinished": "2023-02-15 02:53:16.643",
      "status": "FINISHED"
    },
    {
      "title": "Cache",
      "text": "%spark\n\n// Memory only cache\ngithubDF.cache()\n\n",
      "user": "anonymous",
      "dateUpdated": "2023-02-15 02:53:24.868",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "fontSize": 12.0,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mres3\u001b[0m: \u001b[1m\u001b[32mgithubDF.type\u001b[0m \u003d [actor: struct\u003cavatar_url: string, gravatar_id: string ... 3 more fields\u003e, created_at: string ... 6 more fields]\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1640928476761_1855538996",
      "id": "paragraph_1640928476761_1855538996",
      "dateCreated": "2021-12-31 05:27:56.761",
      "dateStarted": "2023-02-15 02:53:24.869",
      "dateFinished": "2023-02-15 02:53:24.984",
      "status": "FINISHED"
    },
    {
      "title": "persist",
      "text": "%spark\n\n// persist() â€“ without argument. When called without argument, calls cache() internally.\ngithubDF.persist()\n",
      "user": "anonymous",
      "dateUpdated": "2023-02-15 02:53:28.828",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "fontSize": 12.0,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mres4\u001b[0m: \u001b[1m\u001b[32mgithubDF.type\u001b[0m \u003d [actor: struct\u003cavatar_url: string, gravatar_id: string ... 3 more fields\u003e, created_at: string ... 6 more fields]\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1640927227797_111033212",
      "id": "paragraph_1640927227797_111033212",
      "dateCreated": "2021-12-31 05:07:07.797",
      "dateStarted": "2023-02-15 02:53:28.831",
      "dateFinished": "2023-02-15 02:53:28.983",
      "status": "FINISHED"
    },
    {
      "title": "MEMORY_ONLY",
      "text": "%spark\n\nimport org.apache.spark.storage.StorageLevel\ngithubDF.persist(StorageLevel.MEMORY_ONLY)\ngithubDF.foreach(_ \u003d\u003e ())\n",
      "user": "anonymous",
      "dateUpdated": "2023-02-15 02:53:39.589",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "fontSize": 12.0,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "import org.apache.spark.storage.StorageLevel\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://resourcemanager:8088/proxy/application_1676427769595_0004//jobs/job?id\u003d5"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1640927354996_55036034",
      "id": "paragraph_1640927354996_55036034",
      "dateCreated": "2021-12-31 05:09:14.996",
      "dateStarted": "2023-02-15 02:53:39.591",
      "dateFinished": "2023-02-15 02:53:46.549",
      "status": "FINISHED"
    },
    {
      "title": "MEMORY_AND_DISK",
      "text": "%spark\n\nimport org.apache.spark.storage.StorageLevel\ngithubDF.persist(StorageLevel.MEMORY_AND_DISK)\ngithubDF.foreach(_ \u003d\u003e ())\n",
      "user": "anonymous",
      "dateUpdated": "2021-12-31 05:18:20.714",
      "progress": 25,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "fontSize": 12.0,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "import org.apache.spark.storage.StorageLevel\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://zeppelin:4040/jobs"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1640927689802_1729515",
      "id": "paragraph_1640927689802_1729515",
      "dateCreated": "2021-12-31 05:14:49.802",
      "dateStarted": "2021-12-31 05:16:40.576",
      "dateFinished": "2021-12-31 05:16:44.997",
      "status": "FINISHED"
    },
    {
      "title": "MEMORY_ONLY_SER",
      "text": "%spark\n\nimport org.apache.spark.storage.StorageLevel\n\ngithubDF.unpersist(blocking \u003d true)\ngithubDF.persist(StorageLevel.MEMORY_ONLY_SER)\ngithubDF.foreach(_ \u003d\u003e ())",
      "user": "anonymous",
      "dateUpdated": "2021-12-31 05:22:08.850",
      "progress": 50,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "fontSize": 12.0,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "import org.apache.spark.storage.StorageLevel\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://zeppelin:4040/jobs"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1640927838125_42308060",
      "id": "paragraph_1640927838125_42308060",
      "dateCreated": "2021-12-31 05:17:18.125",
      "dateStarted": "2021-12-31 05:22:08.854",
      "dateFinished": "2021-12-31 05:22:23.034",
      "status": "FINISHED"
    },
    {
      "title": "MEMORY_AND_DISK_SER (Java and Scala)",
      "text": "%spark\n\nimport org.apache.spark.storage.StorageLevel\n\ngithubDF.unpersist(blocking \u003d true)\ngithubDF.persist(StorageLevel.MEMORY_AND_DISK_SER)\ngithubDF.foreach(_ \u003d\u003e ())",
      "user": "anonymous",
      "dateUpdated": "2021-12-31 05:29:25.437",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "fontSize": 12.0,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u003cconsole\u003e:31: \u001b[31merror: \u001b[0mnot found: value githubDF\n       githubDF.foreach(_ \u003d\u003e ())\n       ^\n\u003cconsole\u003e:27: \u001b[31merror: \u001b[0mnot found: value githubDF\n       githubDF.unpersist(blocking \u003d true)\n       ^\n\u003cconsole\u003e:27: \u001b[31merror: \u001b[0mnot found: value blocking\n       githubDF.unpersist(blocking \u003d true)\n                          ^\n\u003cconsole\u003e:28: \u001b[31merror: \u001b[0mnot found: value githubDF\n       githubDF.persist(StorageLevel.MEMORY_AND_DISK_SER)\n       ^\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1640928289137_210126880",
      "id": "paragraph_1640928289137_210126880",
      "dateCreated": "2021-12-31 05:24:49.137",
      "dateStarted": "2021-12-31 05:29:25.439",
      "dateFinished": "2021-12-31 05:29:55.527",
      "status": "ABORT"
    },
    {
      "text": "%spark\n\ntopContributorsJoinedDF.explain()",
      "user": "anonymous",
      "dateUpdated": "2021-12-31 05:04:27.115",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "fontSize": 12.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u003d\u003d Physical Plan \u003d\u003d\nBroadcastHashJoin [actor#405.login], [login#427], Inner, BuildRight\n:- Project [actor#405, created_at#406, id#407, org#408, payload#409, public#410, repo#411, type#412]\n:  +- Filter isnotnull(actor#405)\n:     +- FileScan json [actor#405,created_at#406,id#407,org#408,payload#409,public#410,repo#411,type#412] Batched: false, Format: JSON, Location: InMemoryFileIndex[hdfs://namenode:9000/datasets/github.json], PartitionFilters: [], PushedFilters: [IsNotNull(actor)], ReadSchema: struct\u003cactor:struct\u003cavatar_url:string,gravatar_id:string,id:bigint,login:string,url:string\u003e,creat...\n+- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]))\n   +- *(1) Project [login#427]\n      +- *(1) Filter isnotnull(login#427)\n         +- *(1) FileScan json [login#427] Batched: false, Format: JSON, Location: InMemoryFileIndex[hdfs://namenode:9000/datasets/github-top20.json], PartitionFilters: [], PushedFilters: [IsNotNull(login)], ReadSchema: struct\u003clogin:string\u003e\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1640927067115_617514774",
      "id": "paragraph_1640914777382_654592376",
      "dateCreated": "2021-12-31 05:04:27.115",
      "status": "READY"
    },
    {
      "title": "Enable Brodcast Join",
      "text": "%spark\n\n//Renable automatic broadcasting\nspark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\",1024*1024*10)\n\n",
      "user": "anonymous",
      "dateUpdated": "2023-02-15 02:54:58.154",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "fontSize": 10.0,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1640927067115_2068557310",
      "id": "paragraph_1640889373106_2046246779",
      "dateCreated": "2021-12-31 05:04:27.115",
      "dateStarted": "2023-02-15 02:54:58.158",
      "dateFinished": "2023-02-15 02:54:58.274",
      "status": "FINISHED"
    },
    {
      "text": "%spark\n\n// Loading github.json into a dataframe\nval githubDF \u003d spark.read.json(\"hdfs://namenode:9000/datasets/github.json\")\n//githubDF.printSchema\n//githubDF.show()\n\n\n// Loading github-top20.json into a dataframe\nval githubTop20DF \u003d spark.read.json(\"hdfs://namenode:9000/datasets/github-top20.json\")\n//githubTop20DF.printSchema\n//githubTop20DF.show()\n\n\n// Or you can specify the join condition explicitly in case the key is different between tables\nval topContributorsJoinedDF \u003d githubDF.join(githubTop20DF, githubDF(\"actor.login\") \u003d\u003d\u003d githubTop20DF(\"login\"))\n\nimport spark.implicits._\n\n// Counting the results, and ordering by descending count.\ntopContributorsJoinedDF.groupBy(\"actor.login\").count().orderBy(col(\"count\").desc).show()\n//topContributorsJoinedDF.groupBy(\"actor.login\").count.orderBy(\u0027count.desc).show()\n",
      "user": "anonymous",
      "dateUpdated": "2023-02-15 02:54:59.798",
      "progress": 100,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "fontSize": 12.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+------------------+-----+\n|             login|count|\n+------------------+-----+\n|GoogleCodeExporter| 2073|\n|         stackmutt|  284|\n|      greatfirebot|  192|\n|diversify-exp-user|  146|\n|            kwurst|   92|\n|   direwolf-github|   88|\n|     KenanSulayman|   72|\n|        jack-oquin|   52|\n|        manuelrp07|   45|\n|    mirror-updates|   42|\n|     tryton-mirror|   37|\n|        EstifanosG|   32|\n|           houndci|   30|\n|      jeff1evesque|   29|\n|      LukasReschke|   28|\n|       nwt-patrick|   27|\n|           Somasis|   27|\n|        mikegazdag|   26|\n|       tterrag1098|   23|\n|   EmanueleMinotto|   22|\n+------------------+-----+\n\n\u001b[1m\u001b[34mgithubDF\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m \u003d [actor: struct\u003cavatar_url: string, gravatar_id: string ... 3 more fields\u003e, created_at: string ... 6 more fields]\n\u001b[1m\u001b[34mgithubTop20DF\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m \u003d [login: string]\n\u001b[1m\u001b[34mtopContributorsJoinedDF\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m \u003d [actor: struct\u003cavatar_url: string, gravatar_id: string ... 3 more fields\u003e, created_at: string ... 7 more fields]\nimport spark.implicits._\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://resourcemanager:8088/proxy/application_1676427769595_0004//jobs/job?id\u003d6"
            },
            {
              "jobUrl": "http://resourcemanager:8088/proxy/application_1676427769595_0004//jobs/job?id\u003d7"
            },
            {
              "jobUrl": "http://resourcemanager:8088/proxy/application_1676427769595_0004//jobs/job?id\u003d9"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1640927067115_1747321690",
      "id": "paragraph_1640890399555_1395854605",
      "dateCreated": "2021-12-31 05:04:27.115",
      "dateStarted": "2023-02-15 02:54:59.802",
      "dateFinished": "2023-02-15 02:55:04.393",
      "status": "FINISHED"
    },
    {
      "text": "%spark\n\ntopContributorsJoinedDF.explain()\n\n",
      "user": "anonymous",
      "dateUpdated": "2021-12-31 05:04:27.115",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "fontSize": 12.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u003d\u003d Physical Plan \u003d\u003d\nBroadcastHashJoin [actor#405.login], [login#427], Inner, BuildRight\n:- Project [actor#405, created_at#406, id#407, org#408, payload#409, public#410, repo#411, type#412]\n:  +- Filter isnotnull(actor#405)\n:     +- FileScan json [actor#405,created_at#406,id#407,org#408,payload#409,public#410,repo#411,type#412] Batched: false, Format: JSON, Location: InMemoryFileIndex[hdfs://namenode:9000/datasets/github.json], PartitionFilters: [], PushedFilters: [IsNotNull(actor)], ReadSchema: struct\u003cactor:struct\u003cavatar_url:string,gravatar_id:string,id:bigint,login:string,url:string\u003e,creat...\n+- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]))\n   +- *(1) Project [login#427]\n      +- *(1) Filter isnotnull(login#427)\n         +- *(1) FileScan json [login#427] Batched: false, Format: JSON, Location: InMemoryFileIndex[hdfs://namenode:9000/datasets/github-top20.json], PartitionFilters: [], PushedFilters: [IsNotNull(login)], ReadSchema: struct\u003clogin:string\u003e\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1640927067115_660197280",
      "id": "paragraph_1640890439987_243762451",
      "dateCreated": "2021-12-31 05:04:27.115",
      "status": "READY"
    },
    {
      "text": "%spark\n",
      "user": "anonymous",
      "dateUpdated": "2021-12-31 05:04:27.115",
      "progress": 0,
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1640927067115_449823056",
      "id": "paragraph_1640914842278_627600917",
      "dateCreated": "2021-12-31 05:04:27.115",
      "status": "READY"
    }
  ],
  "name": "cache_and_persist",
  "id": "2GSQSK2QB",
  "defaultInterpreterGroup": "spark",
  "version": "0.10.0",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {},
  "config": {
    "isZeppelinNotebookCronEnable": false
  },
  "info": {}
}