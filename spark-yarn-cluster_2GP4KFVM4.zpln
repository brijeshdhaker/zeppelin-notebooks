{
  "paragraphs": [
    {
      "text": "%spark.conf\n\n# Must set SPARK_HOME for this example, because it won\u0027t work for Zeppelin\u0027s embedded spark mode. The embedded spark mode doesn\u0027t \n# use spark-submit to launch spark interpreter, so spark.jars and spark.jars.packages won\u0027t take affect. \n\n#\n#\n#\n\nSPARK_HOME /opt/spark\n\n\n#\n# set execution mode\n#\n\nmaster yarn\n# set driver memory to 8g\nspark.driver.memory 640m\nspark.driver.cores 1\n\n# set executor number to be \nspark.executor.instances  2\nspark.executor.memory  640m\nspark.executor.cores 2\n\n#\nspark.submit.deployMode cluster\n\n#\n# spark.jars can be used for adding any local jar files into spark interpreter\n# \n\n# spark.jars  file:///opt/cloudera/parcels/CDH-6.3.2-1.cdh6.3.2.p0.1605554/lib/hive/lib/hive-hbase-handler-2.1.1-cdh6.3.2.jar\n\n#\n# spark.driver.extraClassPath file:///opt/cloudera/parcels/CDH-6.3.2-1.cdh6.3.2.p0.1605554/lib/hive/lib/hive-hbase-handler-2.1.1-cdh6.3.2.jar\n\n#\n# spark.executor.extraClassPath file:///opt/cloudera/parcels/CDH-6.3.2-1.cdh6.3.2.p0.1605554/lib/hive/lib/hive-hbase-handler-2.1.1-cdh6.3.2.jar\n# spark.executor.extraLibrary file:///opt/cloudera/parcels/CDH-6.3.2-1.cdh6.3.2.p0.1605554/lib/hive/lib/hive-hbase-handler-2.1.1-cdh6.3.2.jar\n\n\n#\n# spark.jars.packages can be used for adding packages into spark interpreter\n# The following is to add avro into your spark interpreter\n#\n\n# spark.jars.packages com.databricks:spark-avro_2.11:4.0.0\n\n#\nspark.yarn.archive\thdfs://namenode:9000/archives/spark-3.1.2.zip\n\n#\n# Python 3.7 Runtime\n#\nspark.yarn.dist.archives hdfs://namenode:9000/archives/pyspark3.7-20221125.tar.gz#environment\n\n\n# com.databricks:spark-avro_2.11:4.0.0\n# io.delta:delta-core_2.12:1.0.0\n# org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2\n\n# spark.jars.packages io.delta:delta-core_2.12:1.0.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2\n# spark.sql.extensions\tio.delta.sql.DeltaSparkSessionExtension\t\n# spark.sql.catalog.spark_catalog\torg.apache.spark.sql.delta.catalog.DeltaCatalog\n# spark.yarn.queue\tengineering\n\n\nspark.yarn.queue\u003dengineering\n\n#\n# spark.kerberos.principal brijeshdhaker@SANDBOX.NET\n# spark.kerberos.keytab /etc/kerberos/users/brijeshdhaker.keytab\n# spark.yarn.am.extraJavaOptions -Djava.security.krb5.conf\u003d/etc/kerberos/krb5.conf\n# spark.driver.extraJavaOptions -Djava.security.krb5.conf\u003d/etc/kerberos/krb5.conf\n# spark.executor.extraJavaOptions -Djava.security.krb5.conf\u003d/etc/kerberos/krb5.conf\n\n",
      "user": "anonymous",
      "dateUpdated": "2023-07-23 01:44:35.809",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "text",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/text",
        "fontSize": 12.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1638439801256_1921713656",
      "id": "paragraph_1638439801256_1921713656",
      "dateCreated": "2021-12-02 15:40:01.256",
      "dateStarted": "2023-07-23 01:44:35.816",
      "dateFinished": "2023-07-23 01:44:35.824",
      "status": "FINISHED"
    },
    {
      "text": "%spark\n\nspark.sparkContext.master",
      "user": "anonymous",
      "dateUpdated": "2023-07-23 01:44:45.843",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "fontSize": 12.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mres1\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m \u003d yarn\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1638440008618_2040289727",
      "id": "paragraph_1638440008618_2040289727",
      "dateCreated": "2021-12-02 15:43:28.618",
      "dateStarted": "2023-07-23 01:44:45.847",
      "dateFinished": "2023-07-23 01:45:05.822",
      "status": "FINISHED"
    },
    {
      "text": "%spark\n\nval employee \u003d Seq(\n      (101,\"Chloe\",-1,\"2018\",8,\"M\",3000),\n      (102,\"Paul\",101,\"2010\",3,\"M\",4000),\n      (103,\"John\",101,\"2010\",1,\"M\",1000),\n      (104,\"Lisa\",102,\"2005\",2,\"F\",2000),\n      (105,\"Evan\",102,\"2010\",7,\"\",-1),\n      (106,\"Amy\",102,\"2010\",9,\"\",-1)\n    )\nval empColumns \u003d Seq(\"id\",\"name\",\"superior_emp_id\",\"year_joined\",\"deptno\",\"gender\",\"salary\")\n\nval department \u003d Seq(\n      (\"Marketing\",1),\n      (\"Sales\",2),\n      (\"Engineering\",3),\n      (\"Finance\",4),\n      (\"IT\",5),\n      (\"HR\",6)\n    )\nval deptColumns \u003d Seq(\"dept_name\",\"deptno\")\n\nimport spark.sqlContext.implicits._\n\nspark.conf.set(\"spark.sql.crossJoin.enabled\",true);\n\nval employeeDF \u003d employee.toDF(empColumns:_*)\nemployeeDF.show(false)\n\nval departmentDF \u003d department.toDF(deptColumns:_*)\ndepartmentDF.show(false)\n\nprintln(\"1. Inner Join Using Expression : join(right: Dataset[_]): DataFrame\")\nemployeeDF.join(departmentDF).show()\n",
      "user": "anonymous",
      "dateUpdated": "2023-07-23 01:45:23.462",
      "progress": 100,
      "config": {
        "colWidth": 12.0,
        "fontSize": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+---+-----+---------------+-----------+------+------+------+\n|id |name |superior_emp_id|year_joined|deptno|gender|salary|\n+---+-----+---------------+-----------+------+------+------+\n|101|Chloe|-1             |2018       |8     |M     |3000  |\n|102|Paul |101            |2010       |3     |M     |4000  |\n|103|John |101            |2010       |1     |M     |1000  |\n|104|Lisa |102            |2005       |2     |F     |2000  |\n|105|Evan |102            |2010       |7     |      |-1    |\n|106|Amy  |102            |2010       |9     |      |-1    |\n+---+-----+---------------+-----------+------+------+------+\n\n+-----------+------+\n|dept_name  |deptno|\n+-----------+------+\n|Marketing  |1     |\n|Sales      |2     |\n|Engineering|3     |\n|Finance    |4     |\n|IT         |5     |\n|HR         |6     |\n+-----------+------+\n\n1. Inner Join Using Expression : join(right: Dataset[_]): DataFrame\n+---+-----+---------------+-----------+------+------+------+-----------+------+\n| id| name|superior_emp_id|year_joined|deptno|gender|salary|  dept_name|deptno|\n+---+-----+---------------+-----------+------+------+------+-----------+------+\n|101|Chloe|             -1|       2018|     8|     M|  3000|  Marketing|     1|\n|101|Chloe|             -1|       2018|     8|     M|  3000|      Sales|     2|\n|101|Chloe|             -1|       2018|     8|     M|  3000|Engineering|     3|\n|101|Chloe|             -1|       2018|     8|     M|  3000|    Finance|     4|\n|101|Chloe|             -1|       2018|     8|     M|  3000|         IT|     5|\n|101|Chloe|             -1|       2018|     8|     M|  3000|         HR|     6|\n|102| Paul|            101|       2010|     3|     M|  4000|  Marketing|     1|\n|102| Paul|            101|       2010|     3|     M|  4000|      Sales|     2|\n|102| Paul|            101|       2010|     3|     M|  4000|Engineering|     3|\n|102| Paul|            101|       2010|     3|     M|  4000|    Finance|     4|\n|102| Paul|            101|       2010|     3|     M|  4000|         IT|     5|\n|102| Paul|            101|       2010|     3|     M|  4000|         HR|     6|\n|103| John|            101|       2010|     1|     M|  1000|  Marketing|     1|\n|103| John|            101|       2010|     1|     M|  1000|      Sales|     2|\n|103| John|            101|       2010|     1|     M|  1000|Engineering|     3|\n|103| John|            101|       2010|     1|     M|  1000|    Finance|     4|\n|103| John|            101|       2010|     1|     M|  1000|         IT|     5|\n|103| John|            101|       2010|     1|     M|  1000|         HR|     6|\n|104| Lisa|            102|       2005|     2|     F|  2000|  Marketing|     1|\n|104| Lisa|            102|       2005|     2|     F|  2000|      Sales|     2|\n+---+-----+---------------+-----------+------+------+------+-----------+------+\nonly showing top 20 rows\n\n\u001b[1m\u001b[34memployee\u001b[0m: \u001b[1m\u001b[32mSeq[(Int, String, Int, String, Int, String, Int)]\u001b[0m \u003d List((101,Chloe,-1,2018,8,M,3000), (102,Paul,101,2010,3,M,4000), (103,John,101,2010,1,M,1000), (104,Lisa,102,2005,2,F,2000), (105,Evan,102,2010,7,\"\",-1), (106,Amy,102,2010,9,\"\",-1))\n\u001b[1m\u001b[34mempColumns\u001b[0m: \u001b[1m\u001b[32mSeq[String]\u001b[0m \u003d List(id, name, superior_emp_id, year_joined, deptno, gender, salary)\n\u001b[1m\u001b[34mdepartment\u001b[0m: \u001b[1m\u001b[32mSeq[(String, Int)]\u001b[0m \u003d List((Marketing,1), (Sales,2), (Engineering,3), (Finance,4), (IT,5), (HR,6))\n\u001b[1m\u001b[34mdeptColumns\u001b[0m: \u001b[1m\u001b[32mSeq[String]\u001b[0m \u003d List(dept_name, deptno)\nimport spark.sqlContext.implicits._\n\u001b[1m\u001b[34memployeeDF\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m \u003d [id: int, name: string ... 5 more fields]\n\u001b[1m\u001b[34mdepartmentDF\u001b[0m: \u001b[1m\u001b[32morg.apac...\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://resourcemanager:8088/proxy/application_1690076238010_0002//jobs/job?id\u003d1"
            },
            {
              "jobUrl": "http://resourcemanager:8088/proxy/application_1690076238010_0002//jobs/job?id\u003d2"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1638442347634_2006643863",
      "id": "paragraph_1638442347634_2006643863",
      "dateCreated": "2021-12-02 16:22:27.634",
      "dateStarted": "2023-07-23 01:45:23.464",
      "dateFinished": "2023-07-23 01:45:28.834",
      "status": "FINISHED"
    },
    {
      "text": "%spark\n\nspark.sparkContext.stop()\n",
      "user": "anonymous",
      "dateUpdated": "2021-12-30 03:15:45.344",
      "progress": 0,
      "config": {
        "colWidth": 12.0,
        "fontSize": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1640794966503_1048083866",
      "id": "paragraph_1640794966503_1048083866",
      "dateCreated": "2021-12-29 16:22:46.503",
      "dateStarted": "2021-12-30 03:15:45.346",
      "dateFinished": "2021-12-30 03:15:45.661",
      "status": "FINISHED"
    },
    {
      "text": "%spark\n",
      "user": "anonymous",
      "dateUpdated": "2023-07-20 02:36:59.046",
      "progress": 0,
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "org.apache.zeppelin.interpreter.InterpreterException: java.io.IOException: Fail to launch interpreter process:\nInterpreter download command: /usr/lib/jvm/java-8-openjdk-amd64/bin/java -Dfile.encoding\u003dUTF-8 -Dlog4j.configuration\u003dlog4j_yarn_cluster.properties -Dzeppelin.log.file\u003d/apps/sandbox/zeppelin/logs/zeppelin-interpreter-spark-shared_process--zeppelin.sandbox.net.log -cp :/opt/zeppelin/interpreter/spark/*:::/opt/zeppelin/interpreter/zeppelin-interpreter-shaded-0.10.1.jar:/opt/zeppelin/interpreter/spark/spark-interpreter-0.10.1.jar:/opt/hadoop/etc/hadoop org.apache.zeppelin.interpreter.remote.RemoteInterpreterDownloader 172.18.0.12 40717 spark /opt/zeppelin/local-repo/spark\nlog4j:WARN No appenders could be found for logger (org.apache.zeppelin.interpreter.remote.RemoteInterpreterDownloader).\nlog4j:WARN Please initialize the log4j system properly.\nlog4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\n[INFO] Interpreter launch command: /opt/spark/bin/spark-submit --class org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer --driver-java-options  -Dfile.encoding\u003dUTF-8 -Dlog4j.configuration\u003dlog4j_yarn_cluster.properties -Dzeppelin.log.file\u003d/apps/sandbox/zeppelin/logs/zeppelin-interpreter-spark-shared_process--zeppelin.sandbox.net.log --conf spark.executor.memory\u003d640m --conf spark.yarn.historyServer.allowTracking\u003dtrue --conf spark.files\u003d/opt/zeppelin/conf/log4j_yarn_cluster.properties --conf spark.yarn.dist.archives\u003d/opt/spark/R/lib/sparkr.zip#sparkr --conf spark.yarn.isPython\u003dtrue --conf spark.jars\u003d/opt/zeppelin/interpreter/spark/scala-2.12/spark-scala-2.12-0.10.1.jar,/opt/zeppelin/interpreter/zeppelin-interpreter-shaded-0.10.1.jar --conf spark.driver.memory\u003d640m --conf spark.submit.deployMode\u003dcluster --conf spark.eventLog.enabled\u003dtrue --conf spark.yarn.maxAppAttempts\u003d1 --conf spark.yarn.queue\u003dengineering --conf spark.yarn.submit.waitAppCompletion\u003dfalse --conf spark.master\u003dyarn --conf spark.executor.cores\u003d2 --conf spark.webui.yarn.useProxy\u003dtrue --conf spark.executor.instances\u003d2 --conf spark.app.name\u003dzeppelin-app --conf spark.driver.cores\u003d1 --conf spark.eventLog.dir\u003dhdfs://namenode:9000/apps/var/log/spark /opt/zeppelin/interpreter/spark/spark-interpreter-0.10.1.jar 172.18.0.12 40717 spark-shared_process :\n23/07/20 02:37:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nlog4j:WARN No appenders could be found for logger (org.apache.hadoop.service.AbstractService).\nlog4j:WARN Please initialize the log4j system properly.\nlog4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\nException in thread \"main\" java.lang.NoClassDefFoundError: com/sun/jersey/api/client/config/ClientConfig\n\tat org.apache.hadoop.yarn.client.api.TimelineClient.createTimelineClient(TimelineClient.java:55)\n\tat org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.createTimelineClient(YarnClientImpl.java:181)\n\tat org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.serviceInit(YarnClientImpl.java:168)\n\tat org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)\n\tat org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:175)\n\tat org.apache.spark.deploy.yarn.Client.run(Client.scala:1227)\n\tat org.apache.spark.deploy.yarn.YarnClusterApplication.start(Client.scala:1634)\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:951)\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1039)\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1048)\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\nCaused by: java.lang.ClassNotFoundException: com.sun.jersey.api.client.config.ClientConfig\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n\tat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n\t... 14 more\n\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreter.open(RemoteInterpreter.java:129)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreter.getFormType(RemoteInterpreter.java:271)\n\tat org.apache.zeppelin.notebook.Paragraph.jobRun(Paragraph.java:438)\n\tat org.apache.zeppelin.notebook.Paragraph.jobRun(Paragraph.java:69)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:172)\n\tat org.apache.zeppelin.scheduler.AbstractScheduler.runJob(AbstractScheduler.java:132)\n\tat org.apache.zeppelin.scheduler.RemoteScheduler$JobRunner.run(RemoteScheduler.java:182)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.io.IOException: Fail to launch interpreter process:\nInterpreter download command: /usr/lib/jvm/java-8-openjdk-amd64/bin/java -Dfile.encoding\u003dUTF-8 -Dlog4j.configuration\u003dlog4j_yarn_cluster.properties -Dzeppelin.log.file\u003d/apps/sandbox/zeppelin/logs/zeppelin-interpreter-spark-shared_process--zeppelin.sandbox.net.log -cp :/opt/zeppelin/interpreter/spark/*:::/opt/zeppelin/interpreter/zeppelin-interpreter-shaded-0.10.1.jar:/opt/zeppelin/interpreter/spark/spark-interpreter-0.10.1.jar:/opt/hadoop/etc/hadoop org.apache.zeppelin.interpreter.remote.RemoteInterpreterDownloader 172.18.0.12 40717 spark /opt/zeppelin/local-repo/spark\nlog4j:WARN No appenders could be found for logger (org.apache.zeppelin.interpreter.remote.RemoteInterpreterDownloader).\nlog4j:WARN Please initialize the log4j system properly.\nlog4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\n[INFO] Interpreter launch command: /opt/spark/bin/spark-submit --class org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer --driver-java-options  -Dfile.encoding\u003dUTF-8 -Dlog4j.configuration\u003dlog4j_yarn_cluster.properties -Dzeppelin.log.file\u003d/apps/sandbox/zeppelin/logs/zeppelin-interpreter-spark-shared_process--zeppelin.sandbox.net.log --conf spark.executor.memory\u003d640m --conf spark.yarn.historyServer.allowTracking\u003dtrue --conf spark.files\u003d/opt/zeppelin/conf/log4j_yarn_cluster.properties --conf spark.yarn.dist.archives\u003d/opt/spark/R/lib/sparkr.zip#sparkr --conf spark.yarn.isPython\u003dtrue --conf spark.jars\u003d/opt/zeppelin/interpreter/spark/scala-2.12/spark-scala-2.12-0.10.1.jar,/opt/zeppelin/interpreter/zeppelin-interpreter-shaded-0.10.1.jar --conf spark.driver.memory\u003d640m --conf spark.submit.deployMode\u003dcluster --conf spark.eventLog.enabled\u003dtrue --conf spark.yarn.maxAppAttempts\u003d1 --conf spark.yarn.queue\u003dengineering --conf spark.yarn.submit.waitAppCompletion\u003dfalse --conf spark.master\u003dyarn --conf spark.executor.cores\u003d2 --conf spark.webui.yarn.useProxy\u003dtrue --conf spark.executor.instances\u003d2 --conf spark.app.name\u003dzeppelin-app --conf spark.driver.cores\u003d1 --conf spark.eventLog.dir\u003dhdfs://namenode:9000/apps/var/log/spark /opt/zeppelin/interpreter/spark/spark-interpreter-0.10.1.jar 172.18.0.12 40717 spark-shared_process :\n23/07/20 02:37:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nlog4j:WARN No appenders could be found for logger (org.apache.hadoop.service.AbstractService).\nlog4j:WARN Please initialize the log4j system properly.\nlog4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\nException in thread \"main\" java.lang.NoClassDefFoundError: com/sun/jersey/api/client/config/ClientConfig\n\tat org.apache.hadoop.yarn.client.api.TimelineClient.createTimelineClient(TimelineClient.java:55)\n\tat org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.createTimelineClient(YarnClientImpl.java:181)\n\tat org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.serviceInit(YarnClientImpl.java:168)\n\tat org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)\n\tat org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:175)\n\tat org.apache.spark.deploy.yarn.Client.run(Client.scala:1227)\n\tat org.apache.spark.deploy.yarn.YarnClusterApplication.start(Client.scala:1634)\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:951)\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1039)\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1048)\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\nCaused by: java.lang.ClassNotFoundException: com.sun.jersey.api.client.config.ClientConfig\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n\tat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n\t... 14 more\n\n\tat org.apache.zeppelin.interpreter.remote.ExecRemoteInterpreterProcess.start(ExecRemoteInterpreterProcess.java:97)\n\tat org.apache.zeppelin.interpreter.ManagedInterpreterGroup.getOrCreateInterpreterProcess(ManagedInterpreterGroup.java:68)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreter.getOrCreateInterpreterProcess(RemoteInterpreter.java:104)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreter.internal_create(RemoteInterpreter.java:154)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreter.open(RemoteInterpreter.java:126)\n\t... 13 more\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1640834145345_1976143351",
      "id": "paragraph_1640834145345_1976143351",
      "dateCreated": "2021-12-30 03:15:45.345",
      "dateStarted": "2023-07-20 02:36:59.048",
      "dateFinished": "2023-07-20 02:37:01.205",
      "status": "ERROR"
    },
    {
      "text": "%spark\n",
      "user": "anonymous",
      "dateUpdated": "2023-07-20 02:36:59.048",
      "progress": 0,
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1689820619048_1152570598",
      "id": "paragraph_1689820619048_1152570598",
      "dateCreated": "2023-07-20 02:36:59.048",
      "status": "READY"
    }
  ],
  "name": "spark-yarn-cluster",
  "id": "2GP4KFVM4",
  "defaultInterpreterGroup": "spark",
  "version": "0.10.0",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {},
  "config": {
    "isZeppelinNotebookCronEnable": false
  },
  "info": {}
}