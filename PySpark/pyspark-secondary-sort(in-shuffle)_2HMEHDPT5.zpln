{
  "paragraphs": [
    {
      "title": "Secondary Sort ",
      "text": "%md\n\n`Secondary sort` is nothing more than sorting data by two values. What makes the problem challenging is sorting large volumes of data as fast as possible without running out of memory and thus crashing the machines. Thus, Secondary sort represents the process of grouping data by key and sorting the payloads within each group before running further computation\n",
      "user": "anonymous",
      "dateUpdated": "2022-11-28 02:41:43.774",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "fontSize": 12.0,
        "editorHide": true,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003e\u003ccode\u003eSecondary sort\u003c/code\u003e is nothing more than sorting data by two values. What makes the problem challenging is sorting large volumes of data as fast as possible without running out of memory and thus crashing the machines. Thus, Secondary sort represents the process of grouping data by key and sorting the payloads within each group before running further computation\u003c/p\u003e\n\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1669561094954_708676555",
      "id": "paragraph_1669561094954_708676555",
      "dateCreated": "2022-11-27 14:58:14.954",
      "dateStarted": "2022-11-28 02:41:43.775",
      "dateFinished": "2022-11-28 02:41:43.783",
      "status": "FINISHED"
    },
    {
      "title": "Secondary Sort Design Pattern",
      "text": "%md\n\nOne commonly occurring big data problem which can be tackled with secondary sort is sessionization. Sessionization is the process in which one tries to analyze all data which represent a session, such as all the actions executed by a user or all the events generated by a device\n\nTo better illustrate how sessionization relates to secondary sort, one can take the dataset about taxi trips reported in Chicago:\n\nTrip ID\tTaxi ID\tTrip Start Timestamp\tTrip End Timestamp\t...\nb258df504d2fbf85...\tb7b0be6d3ec6c589aeac84c0...\t07/19/2016 02:45:00 PM\t07/19/2016 02:45:00 PM\t...\nb258df6e5ace922...\t0d61c4f9c8cb8d280fce3887...\t01/22/2015 04:15:00 PM\t01/22/2015 04:30:00 PM\t...\n...\t...\t...\t...\t...\n\nSuppose that one has the task to compute the down time of each taxi cab since all time. A session can be mapped as follows: the taxi cab as the analyzed object and the trip series as the events over time(Fig. 1).\n\n![Fig. 1 - Sessions](https://www-cdn.qwertee.io/media/uploads/blog/spark_secondary_sort/sessionization2.png)\n\n\nMore concrete, the analyzed object represented by Taxi ID will become the key, while the rest of the values(Trip ID, Trip Start Timestamp, Trip End Timestamp, etc.) representing the trip series will become the payload(Fig. 2). The problem can further be reduced to: group the dataset by the Taxi ID and sort ascending the payloads within each group according to the Trip Start Timestamp.\n\n![Fig. 2 - Sessions](https://www-cdn.qwertee.io/media/uploads/blog/spark_secondary_sort/groups2.png)\n\nAfter the sort is finished, one can loop over the payloads within each group and sum up the time deltas between each trip.\n\n![Fig. 3 - Delta computation](https://www-cdn.qwertee.io/media/uploads/blog/spark_secondary_sort/deltas2.png)\n\nAs one can see, the solution is reduced to a group by key and sort by another value, which matches exactly the definition of secondary sort.\n\nDepending on the map reduce system, secondary sort can be implemented differently. There are two common strategies for implementing secondary sort[1]:\n\nin reducer sort - for some map reduce systems secondary sort relies on buffering data into each reducer and sorting each group in memory.\nin shuffle sort - other map reduce systems deliver data from mappers to reducers not only grouped by key, but also sorted by key. With this strategy, one can take advantage of the shuffle process to sort data by a second value before delivering it to the reducers.\n",
      "user": "anonymous",
      "dateUpdated": "2022-11-27 15:04:15.649",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "fontSize": 12.0,
        "editorHide": true,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003eOne commonly occurring big data problem which can be tackled with secondary sort is sessionization. Sessionization is the process in which one tries to analyze all data which represent a session, such as all the actions executed by a user or all the events generated by a device\u003c/p\u003e\n\u003cp\u003eTo better illustrate how sessionization relates to secondary sort, one can take the dataset about taxi trips reported in Chicago:\u003c/p\u003e\n\u003cp\u003eTrip ID\tTaxi ID\tTrip Start Timestamp\tTrip End Timestamp\t\u0026hellip;\u003cbr /\u003e\nb258df504d2fbf85\u0026hellip;\tb7b0be6d3ec6c589aeac84c0\u0026hellip;\t07/19/2016 02:45:00 PM\t07/19/2016 02:45:00 PM\t\u0026hellip;\u003cbr /\u003e\nb258df6e5ace922\u0026hellip;\t0d61c4f9c8cb8d280fce3887\u0026hellip;\t01/22/2015 04:15:00 PM\t01/22/2015 04:30:00 PM\t\u0026hellip;\u003cbr /\u003e\n\u0026hellip;\t\u0026hellip;\t\u0026hellip;\t\u0026hellip;\t\u0026hellip;\u003c/p\u003e\n\u003cp\u003eSuppose that one has the task to compute the down time of each taxi cab since all time. A session can be mapped as follows: the taxi cab as the analyzed object and the trip series as the events over time(Fig. 1).\u003c/p\u003e\n\u003cp\u003e\u003cimg src\u003d\"https://www-cdn.qwertee.io/media/uploads/blog/spark_secondary_sort/sessionization2.png\" alt\u003d\"Fig. 1 - Sessions\" /\u003e\u003c/p\u003e\n\u003cp\u003eMore concrete, the analyzed object represented by Taxi ID will become the key, while the rest of the values(Trip ID, Trip Start Timestamp, Trip End Timestamp, etc.) representing the trip series will become the payload(Fig. 2). The problem can further be reduced to: group the dataset by the Taxi ID and sort ascending the payloads within each group according to the Trip Start Timestamp.\u003c/p\u003e\n\u003cp\u003e\u003cimg src\u003d\"https://www-cdn.qwertee.io/media/uploads/blog/spark_secondary_sort/groups2.png\" alt\u003d\"Fig. 2 - Sessions\" /\u003e\u003c/p\u003e\n\u003cp\u003eAfter the sort is finished, one can loop over the payloads within each group and sum up the time deltas between each trip.\u003c/p\u003e\n\u003cp\u003e\u003cimg src\u003d\"https://www-cdn.qwertee.io/media/uploads/blog/spark_secondary_sort/deltas2.png\" alt\u003d\"Fig. 3 - Delta computation\" /\u003e\u003c/p\u003e\n\u003cp\u003eAs one can see, the solution is reduced to a group by key and sort by another value, which matches exactly the definition of secondary sort.\u003c/p\u003e\n\u003cp\u003eDepending on the map reduce system, secondary sort can be implemented differently. There are two common strategies for implementing secondary sort[1]:\u003c/p\u003e\n\u003cp\u003ein reducer sort - for some map reduce systems secondary sort relies on buffering data into each reducer and sorting each group in memory.\u003cbr /\u003e\nin shuffle sort - other map reduce systems deliver data from mappers to reducers not only grouped by key, but also sorted by key. With this strategy, one can take advantage of the shuffle process to sort data by a second value before delivering it to the reducers.\u003c/p\u003e\n\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1669561139114_2090629360",
      "id": "paragraph_1669561139114_2090629360",
      "dateCreated": "2022-11-27 14:58:59.114",
      "dateStarted": "2022-11-27 15:04:15.649",
      "dateFinished": "2022-11-27 15:04:15.665",
      "status": "FINISHED"
    },
    {
      "text": "%md\n\n### In reducer sort\n\nA simple and straightforward strategy is to let the system execute a standard map reduce job. Once the grouped data arrives at each reducer, it will get buffered and sorted in memory. This solution might perform better time wise, but it cannot scale because of the in memory sort. As soon as the group will exceed the reducerâ€™s memory an Out of Memory exception is thrown and the job will fail.\n\n",
      "user": "anonymous",
      "dateUpdated": "2022-11-28 02:41:15.339",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "fontSize": 12.0,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003eIn reducer sort\u003c/h3\u003e\n\u003cp\u003eA simple and straightforward strategy is to let the system execute a standard map reduce job. Once the grouped data arrives at each reducer, it will get buffered and sorted in memory. This solution might perform better time wise, but it cannot scale because of the in memory sort. As soon as the group will exceed the reducerâ€™s memory an Out of Memory exception is thrown and the job will fail.\u003c/p\u003e\n\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1669561459516_219286879",
      "id": "paragraph_1669561459516_219286879",
      "dateCreated": "2022-11-27 15:04:19.516",
      "dateStarted": "2022-11-28 02:41:15.340",
      "dateFinished": "2022-11-28 02:41:15.343",
      "status": "FINISHED"
    },
    {
      "text": "%md\n\n### In shuffle sort\n\nAs mentioned before, some map reduce systems also sort by key before delivering data to the reducers. In order to force the system to also sort by a second value, one has to create a composite key, thus a value from the payload is promoted and is appended to the key. The system shuffle will first sort by the first part of the key and then by the second. This approach of creating a composite key out of the initial key and another value from the payload is known as value to key conversion.\n\nValue to key conversion\n\n![Value to key conversion](https://www-cdn.qwertee.io/media/uploads/blog/spark_secondary_sort/value2key2.png)\n\nFig. 4 - Value to key conversion\n\nThe advantage of using this strategy is that it leverages the systemâ€™s shuffle process in order to sort the payloads, which is very efficient memory wise. The downside is that extra code has to be written to tell the system to group data only by the first part of the composite key before sending it to the reducer.\n\n\n",
      "user": "anonymous",
      "dateUpdated": "2022-11-28 02:41:12.410",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "fontSize": 12.0,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003eIn shuffle sort\u003c/h3\u003e\n\u003cp\u003eAs mentioned before, some map reduce systems also sort by key before delivering data to the reducers. In order to force the system to also sort by a second value, one has to create a composite key, thus a value from the payload is promoted and is appended to the key. The system shuffle will first sort by the first part of the key and then by the second. This approach of creating a composite key out of the initial key and another value from the payload is known as value to key conversion.\u003c/p\u003e\n\u003cp\u003eValue to key conversion\u003c/p\u003e\n\u003cp\u003e\u003cimg src\u003d\"https://www-cdn.qwertee.io/media/uploads/blog/spark_secondary_sort/value2key2.png\" alt\u003d\"Value to key conversion\" /\u003e\u003c/p\u003e\n\u003cp\u003eFig. 4 - Value to key conversion\u003c/p\u003e\n\u003cp\u003eThe advantage of using this strategy is that it leverages the systemâ€™s shuffle process in order to sort the payloads, which is very efficient memory wise. The downside is that extra code has to be written to tell the system to group data only by the first part of the composite key before sending it to the reducer.\u003c/p\u003e\n\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1669560759451_295358164",
      "id": "paragraph_1669560759451_295358164",
      "dateCreated": "2022-11-27 14:52:39.451",
      "dateStarted": "2022-11-28 02:41:12.410",
      "dateFinished": "2022-11-28 02:41:12.414",
      "status": "FINISHED"
    },
    {
      "title": "Spark shuffle sort",
      "text": "%md\n\n![Fig. 5 - repartitionAndSortWithinPartitions](https://www-cdn.qwertee.io/media/uploads/blog/spark_secondary_sort/repartitionandsortwithinpartitions2.png)\n\n\nIn this section one can see a pySpark implementation for secondary sort using the system shuffle process. Spark 1.2 introduced repartitionAndSortWithinPartitions, which allows to repartition the entire dataframe according to a partitioner and also sort data within each partition according to a comparator. In other words, repartitionAndSortWithinPartitions will shuffle(move) all data having the same key to the same partition, while also sorting it according to a comparator. Because both partitioner and the sort comparator are customizable this is ideal for implementing secondary sort.\n\nOne just has to define a composite key, implement a partitioner which only uses the first part of the key and a sort comparator which uses the entire composite key. The system will then put all data that has the same partitioning key on the same partition, in the order given by the composite key.\n\nrepartitionAndSortWithinPartitions is not a group by operation. It will only move data having the same key to the same partition and sort it according to the comparator(Fig. 5). To actually transform the data into groups, one has to iterate through all items of each partition keeping track of when a group ends and the next one begins[4] as one will see shortly.\n\nAs before, the first thing one has to do is to create a pair RDD. For this implementation make_pair will return a pair having a composite key (Taxi ID, Trip Start Timestamp) and a payload, after which the payload is compressed to only two values: Trip Start Timestamp and Trip End Timestamp:\n",
      "user": "anonymous",
      "dateUpdated": "2022-11-27 15:35:19.448",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "fontSize": 12.0,
        "editorHide": true,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003e\u003cimg src\u003d\"https://www-cdn.qwertee.io/media/uploads/blog/spark_secondary_sort/repartitionandsortwithinpartitions2.png\" alt\u003d\"Fig. 5 - repartitionAndSortWithinPartitions\" /\u003e\u003c/p\u003e\n\u003cp\u003eIn this section one can see a pySpark implementation for secondary sort using the system shuffle process. Spark 1.2 introduced repartitionAndSortWithinPartitions, which allows to repartition the entire dataframe according to a partitioner and also sort data within each partition according to a comparator. In other words, repartitionAndSortWithinPartitions will shuffle(move) all data having the same key to the same partition, while also sorting it according to a comparator. Because both partitioner and the sort comparator are customizable this is ideal for implementing secondary sort.\u003c/p\u003e\n\u003cp\u003eOne just has to define a composite key, implement a partitioner which only uses the first part of the key and a sort comparator which uses the entire composite key. The system will then put all data that has the same partitioning key on the same partition, in the order given by the composite key.\u003c/p\u003e\n\u003cp\u003erepartitionAndSortWithinPartitions is not a group by operation. It will only move data having the same key to the same partition and sort it according to the comparator(Fig. 5). To actually transform the data into groups, one has to iterate through all items of each partition keeping track of when a group ends and the next one begins[4] as one will see shortly.\u003c/p\u003e\n\u003cp\u003eAs before, the first thing one has to do is to create a pair RDD. For this implementation make_pair will return a pair having a composite key (Taxi ID, Trip Start Timestamp) and a payload, after which the payload is compressed to only two values: Trip Start Timestamp and Trip End Timestamp:\u003c/p\u003e\n\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1669561875478_1851608218",
      "id": "paragraph_1669561875478_1851608218",
      "dateCreated": "2022-11-27 15:11:15.478",
      "dateStarted": "2022-11-27 15:35:19.448",
      "dateFinished": "2022-11-27 15:35:19.460",
      "status": "FINISHED"
    },
    {
      "text": "%md\n\n### Data Set Columns \n\nhttps://data.cityofchicago.org/Transportation/Taxi-Trips/wrvz-psew/data\n\n| `ID` | `Name` |\n|:--:|:--:|\n|0  | Trip ID  |\n|1  | Taxi ID  |\n|2  | Trip Start Timestamp |\n|3  | Trip End Timestamp |\n|4  | Trip Seconds |\n|5  | Trip Miles |\n|6 | Trip ID | \t\n|7 | Taxi ID | \t\n|8 | Trip Start Timestamp |  \n|9 | Trip End Timestamp |  \t\n|10 | Trip Seconds |  \t\n|11 | Trip Miles |  \n|12 | Pickup Census Tract |  \t\n|13 | Dropoff Census Tract |  \t\n|14 | Pickup Community Area |  \t\n|15 | Dropoff Community Area |  \t\n|16 | Fare | \n|17 | Tips |  \t\n|18 | Tolls |  \t\n|19 | Extras |  \t\n|20 | Trip Total |  \t\n|21 | Payment Type |  \t\n|22 | Company |  \t\n|23 | Pickup Centroid Latitude |  \t\n|24 | Pickup Centroid Longitude |  \t\n|25 | Pickup Centroid Location | \n|26 | Dropoff Centroid Latitude |  \t\n|27 | Dropoff Centroid Longitude |  \t\n|28 | Dropoff Centroid Location | \n\n",
      "user": "anonymous",
      "dateUpdated": "2022-11-28 02:42:30.838",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "fontSize": 12.0,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003eData Set Columns\u003c/h3\u003e\n\u003cp\u003e\u003ca href\u003d\"https://data.cityofchicago.org/Transportation/Taxi-Trips/wrvz-psew/data\"\u003ehttps://data.cityofchicago.org/Transportation/Taxi-Trips/wrvz-psew/data\u003c/a\u003e\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\u003cth align\u003d\"center\"\u003e\u003ccode\u003eID\u003c/code\u003e\u003c/th\u003e\u003cth align\u003d\"center\"\u003e\u003ccode\u003eName\u003c/code\u003e\u003c/th\u003e\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\u003ctd align\u003d\"center\"\u003e0\u003c/td\u003e\u003ctd align\u003d\"center\"\u003eTrip ID\u003c/td\u003e\u003c/tr\u003e\n\u003ctr\u003e\u003ctd align\u003d\"center\"\u003e1\u003c/td\u003e\u003ctd align\u003d\"center\"\u003eTaxi ID\u003c/td\u003e\u003c/tr\u003e\n\u003ctr\u003e\u003ctd align\u003d\"center\"\u003e2\u003c/td\u003e\u003ctd align\u003d\"center\"\u003eTrip Start Timestamp\u003c/td\u003e\u003c/tr\u003e\n\u003ctr\u003e\u003ctd align\u003d\"center\"\u003e3\u003c/td\u003e\u003ctd align\u003d\"center\"\u003eTrip End Timestamp\u003c/td\u003e\u003c/tr\u003e\n\u003ctr\u003e\u003ctd align\u003d\"center\"\u003e4\u003c/td\u003e\u003ctd align\u003d\"center\"\u003eTrip Seconds\u003c/td\u003e\u003c/tr\u003e\n\u003ctr\u003e\u003ctd align\u003d\"center\"\u003e5\u003c/td\u003e\u003ctd align\u003d\"center\"\u003eTrip Miles\u003c/td\u003e\u003c/tr\u003e\n\u003ctr\u003e\u003ctd align\u003d\"center\"\u003e6\u003c/td\u003e\u003ctd align\u003d\"center\"\u003eTrip ID\u003c/td\u003e\u003c/tr\u003e\n\u003ctr\u003e\u003ctd align\u003d\"center\"\u003e7\u003c/td\u003e\u003ctd align\u003d\"center\"\u003eTaxi ID\u003c/td\u003e\u003c/tr\u003e\n\u003ctr\u003e\u003ctd align\u003d\"center\"\u003e8\u003c/td\u003e\u003ctd align\u003d\"center\"\u003eTrip Start Timestamp\u003c/td\u003e\u003c/tr\u003e\n\u003ctr\u003e\u003ctd align\u003d\"center\"\u003e9\u003c/td\u003e\u003ctd align\u003d\"center\"\u003eTrip End Timestamp\u003c/td\u003e\u003c/tr\u003e\n\u003ctr\u003e\u003ctd align\u003d\"center\"\u003e10\u003c/td\u003e\u003ctd align\u003d\"center\"\u003eTrip Seconds\u003c/td\u003e\u003c/tr\u003e\n\u003ctr\u003e\u003ctd align\u003d\"center\"\u003e11\u003c/td\u003e\u003ctd align\u003d\"center\"\u003eTrip Miles\u003c/td\u003e\u003c/tr\u003e\n\u003ctr\u003e\u003ctd align\u003d\"center\"\u003e12\u003c/td\u003e\u003ctd align\u003d\"center\"\u003ePickup Census Tract\u003c/td\u003e\u003c/tr\u003e\n\u003ctr\u003e\u003ctd align\u003d\"center\"\u003e13\u003c/td\u003e\u003ctd align\u003d\"center\"\u003eDropoff Census Tract\u003c/td\u003e\u003c/tr\u003e\n\u003ctr\u003e\u003ctd align\u003d\"center\"\u003e14\u003c/td\u003e\u003ctd align\u003d\"center\"\u003ePickup Community Area\u003c/td\u003e\u003c/tr\u003e\n\u003ctr\u003e\u003ctd align\u003d\"center\"\u003e15\u003c/td\u003e\u003ctd align\u003d\"center\"\u003eDropoff Community Area\u003c/td\u003e\u003c/tr\u003e\n\u003ctr\u003e\u003ctd align\u003d\"center\"\u003e16\u003c/td\u003e\u003ctd align\u003d\"center\"\u003eFare\u003c/td\u003e\u003c/tr\u003e\n\u003ctr\u003e\u003ctd align\u003d\"center\"\u003e17\u003c/td\u003e\u003ctd align\u003d\"center\"\u003eTips\u003c/td\u003e\u003c/tr\u003e\n\u003ctr\u003e\u003ctd align\u003d\"center\"\u003e18\u003c/td\u003e\u003ctd align\u003d\"center\"\u003eTolls\u003c/td\u003e\u003c/tr\u003e\n\u003ctr\u003e\u003ctd align\u003d\"center\"\u003e19\u003c/td\u003e\u003ctd align\u003d\"center\"\u003eExtras\u003c/td\u003e\u003c/tr\u003e\n\u003ctr\u003e\u003ctd align\u003d\"center\"\u003e20\u003c/td\u003e\u003ctd align\u003d\"center\"\u003eTrip Total\u003c/td\u003e\u003c/tr\u003e\n\u003ctr\u003e\u003ctd align\u003d\"center\"\u003e21\u003c/td\u003e\u003ctd align\u003d\"center\"\u003ePayment Type\u003c/td\u003e\u003c/tr\u003e\n\u003ctr\u003e\u003ctd align\u003d\"center\"\u003e22\u003c/td\u003e\u003ctd align\u003d\"center\"\u003eCompany\u003c/td\u003e\u003c/tr\u003e\n\u003ctr\u003e\u003ctd align\u003d\"center\"\u003e23\u003c/td\u003e\u003ctd align\u003d\"center\"\u003ePickup Centroid Latitude\u003c/td\u003e\u003c/tr\u003e\n\u003ctr\u003e\u003ctd align\u003d\"center\"\u003e24\u003c/td\u003e\u003ctd align\u003d\"center\"\u003ePickup Centroid Longitude\u003c/td\u003e\u003c/tr\u003e\n\u003ctr\u003e\u003ctd align\u003d\"center\"\u003e25\u003c/td\u003e\u003ctd align\u003d\"center\"\u003ePickup Centroid Location\u003c/td\u003e\u003c/tr\u003e\n\u003ctr\u003e\u003ctd align\u003d\"center\"\u003e26\u003c/td\u003e\u003ctd align\u003d\"center\"\u003eDropoff Centroid Latitude\u003c/td\u003e\u003c/tr\u003e\n\u003ctr\u003e\u003ctd align\u003d\"center\"\u003e27\u003c/td\u003e\u003ctd align\u003d\"center\"\u003eDropoff Centroid Longitude\u003c/td\u003e\u003c/tr\u003e\n\u003ctr\u003e\u003ctd align\u003d\"center\"\u003e28\u003c/td\u003e\u003ctd align\u003d\"center\"\u003eDropoff Centroid Location\u003c/td\u003e\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1669603345411_488813639",
      "id": "paragraph_1669603345411_488813639",
      "dateCreated": "2022-11-28 02:42:25.411",
      "dateStarted": "2022-11-28 02:42:30.839",
      "dateFinished": "2022-11-28 02:42:30.847",
      "status": "FINISHED"
    },
    {
      "text": "%spark.conf\n\nspark.app.name in-shuffle-secondary-sort-compute\n",
      "user": "anonymous",
      "dateUpdated": "2022-11-28 02:43:29.947",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "text",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/text",
        "fontSize": 12.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1669603373650_247869068",
      "id": "paragraph_1669603373650_247869068",
      "dateCreated": "2022-11-28 02:42:53.650",
      "status": "READY"
    },
    {
      "text": "%pyspark\n\n\n# Spark Context\nsc.version\n\n#\nsc.setLogLevel(\u0027INFO\u0027)",
      "user": "anonymous",
      "dateUpdated": "2022-11-28 02:42:53.486",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "fontSize": 12.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u00273.1.2\u0027\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1669559149891_963536245",
      "id": "paragraph_1669559149891_963536245",
      "dateCreated": "2022-11-27 14:25:49.891",
      "dateStarted": "2022-11-27 15:10:40.009",
      "dateFinished": "2022-11-27 15:10:40.017",
      "status": "FINISHED"
    },
    {
      "text": "%pyspark\n\nimport time\nimport itertools\nimport re\nfrom pyspark.rdd import portable_hash\nfrom datetime import datetime\n\nAPP_NAME   \u003d \u0027in-shuffle-secondary-sort-compute\u0027\nINPUT_FILE \u003d \u0027/datasets/Taxi_Trips.csv\u0027\nOUTPUT_DIR \u003d \u0027/test/output-in-shuffle-sort-compute-{timestamp}.txt\u0027\n\nCOMMA_DELIMITER \u003d re.compile(\u0027\u0027\u0027,(?\u003d(?:[^\"]*\"[^\"]*\")*[^\"]*$)\u0027\u0027\u0027)\n\nFIRST_KEY \u003d 1\nSECOND_KEY \u003d 2\nTRIP_END_TIMESTAMP \u003d 3\n\n\nTIMESTAMP \u003d int(time.time())\n\n\ndef partition_func(key):\n    return portable_hash(key[0])\n\n\ndef key_func(entry):\n    return entry[0], entry[1]\n\n#\n# make_pair will return a pair having a composite key (Taxi ID, Trip Start Timestamp) and a payload, after which the payload is compressed to only two values: Trip Start Timestamp and Trip End Timestamp:\n#\ndef make_pair(entry):\n    key \u003d (entry[FIRST_KEY], entry[SECOND_KEY])\n    return key, entry\n\n\ndef unpair(entry):\n    return entry[0][0], entry[1][0], entry[1][1]\n\n\ndef create_pair_rdd(ctx):\n    rawRDD \u003d ctx.textFile(INPUT_FILE)\n    headerlessRDD \u003d rawRDD.filter(lambda x: not x.startswith(\u0027Trip ID\u0027))\n    rdd \u003d headerlessRDD.map(lambda x: COMMA_DELIMITER.split(x))\n    validRDD \u003d rdd.filter(lambda x: len(x[FIRST_KEY]) \u003e 0 and len(x[SECOND_KEY]) \u003e 0 and len(x[TRIP_END_TIMESTAMP]) \u003e 0)\n    \n    pairRDD \u003d validRDD.map(make_pair)\n    \n    # after which the payload is compressed to only two values: Trip Start Timestamp and Trip End Timestamp:\n    compressedRDD \u003d pairRDD.mapValues(lambda x: (x[SECOND_KEY], x[TRIP_END_TIMESTAMP]))\n\n    return compressedRDD\n\n#\n#\n#\ndef sorted_group(lines):\n    return itertools.groupby(lines, key\u003dlambda x: x[0])\n\n#\n#\n#\ndef calculate_loss(entry):\n    key, group \u003d entry\n    loss \u003d 0\n    _, _, prev_end \u003d next(group)\n\n    for item in group:\n        _, start, end \u003d item\n        delta \u003d datetime.strptime(start, \u0027%m/%d/%Y %I:%M:%S %p\u0027).timestamp() \\\n                - datetime.strptime(prev_end, \u0027%m/%d/%Y %I:%M:%S %p\u0027).timestamp()\n        if delta \u003e 0:\n            loss +\u003d delta\n        prev_end \u003d end\n\n    return key, loss",
      "user": "anonymous",
      "dateUpdated": "2023-02-10 02:40:17.710",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "fontSize": 12.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1669559338876_1926200043",
      "id": "paragraph_1669559338876_1926200043",
      "dateCreated": "2022-11-27 14:28:58.876",
      "dateStarted": "2023-02-10 02:40:17.713",
      "dateFinished": "2023-02-10 02:40:17.719",
      "status": "FINISHED"
    },
    {
      "text": "%pyspark\n\n\n\n\n#\nrdd \u003d create_pair_rdd(sc)\n\n\n#\n# As discussed, repartitionAndSortWithinPartitions receives a partitioner(partition_func) and a sort comparator (keyfunc). \n# One also can specify the number of partitions to use and the sorting order. The partitioner just uses the standard \n# portable_hash function with the first part of the key, while the sort comparator just returns back the composite key. \n#\nsortedRDD \u003d rdd.repartitionAndSortWithinPartitions(partitionFunc\u003dpartition_func,\n                                                   numPartitions\u003d4,\n                                                   keyfunc\u003dkey_func,\n                                                   ascending\u003dTrue)\n#\n# After the shuffling process invoked by repartitionAndSortWithinPartitions is over, one can iterate over each RDD partition in order to group the data as presented in the next code snippet:\n#\nunpairedRDD \u003d sortedRDD.map(unpair, preservesPartitioning\u003dTrue)\n\n#\n# One very important aspect to discuss before moving forward is why itertools.groupby was used and not a regular group by algorithm. Sure, the standard python library is elegant and it does the job of grouping the data, but there is more to it. The itertools.groupby, keeps the iterator received by sorted_group still as an iterator. Keeping it as an iterator and not materializing it to something else saves memory and prevents Spark from going out of memory. This strategy is known as iterator-to-iterator transformation and it has the advantage of chaining multiple data transformations delaying the actual evaluation, thus saving space and time[5]. Using the iterator-to-iterator transformation strategy, one can continue processing the secondary sort result, thus sorted_group just returns now a new iterator obtained from itertools.groupby. This transforms the partitions grouping data by key. Because this is kept as an iterator, nothing is evaluated yet, therefor no extra space is used.\n#\n# For the last step, one can apply a map transformation on the fresh groups in order to iterate over them and compute the total down time.\n#\n\n\n#\ngroupedRDD \u003d unpairedRDD.mapPartitions(sorted_group, preservesPartitioning\u003dTrue)\n\n# The map method receives calculate_loss, which just iterates over each group and computes the total downtime. Elements are accessed one by one, which again does not allocate extra space.\nlossRDD \u003d groupedRDD.map(calculate_loss)\n\n#\nlossRDD.saveAsTextFile(OUTPUT_DIR.format(timestamp\u003dTIMESTAMP))\n",
      "user": "anonymous",
      "dateUpdated": "2023-02-10 02:40:21.442",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "fontSize": 12.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://resourcemanager:8088/proxy/application_1675994320382_0001//jobs/job?id\u003d41"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1669559451993_847979606",
      "id": "paragraph_1669559451993_847979606",
      "dateCreated": "2022-11-27 14:30:51.993",
      "dateStarted": "2023-02-10 02:40:21.444",
      "dateFinished": "2023-02-10 02:41:31.778",
      "status": "FINISHED"
    },
    {
      "text": "%pyspark\n",
      "user": "anonymous",
      "dateUpdated": "2022-11-27 14:41:12.098",
      "progress": 0,
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1669560072098_1558622900",
      "id": "paragraph_1669560072098_1558622900",
      "dateCreated": "2022-11-27 14:41:12.098",
      "status": "READY"
    }
  ],
  "name": "pyspark-secondary-sort(in-shuffle)",
  "id": "2HMEHDPT5",
  "defaultInterpreterGroup": "spark",
  "version": "0.10.1",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {},
  "config": {
    "isZeppelinNotebookCronEnable": false
  },
  "info": {}
}