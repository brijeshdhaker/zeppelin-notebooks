{
  "paragraphs": [
    {
      "title": "Settings for YARN Cluster",
      "text": "%spark.conf\n\nspark.app.name paired-rdd-transformations\n",
      "user": "anonymous",
      "dateUpdated": "2022-12-11 13:01:42.487",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "text",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/text",
        "fontSize": 12.0,
        "results": {},
        "enabled": true,
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "java.io.IOException: Can not change interpreter properties when interpreter process has already been launched\n\tat org.apache.zeppelin.interpreter.InterpreterSetting.setInterpreterGroupProperties(InterpreterSetting.java:945)\n\tat org.apache.zeppelin.interpreter.ConfInterpreter.interpret(ConfInterpreter.java:72)\n\tat org.apache.zeppelin.notebook.Paragraph.jobRun(Paragraph.java:484)\n\tat org.apache.zeppelin.notebook.Paragraph.jobRun(Paragraph.java:69)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:172)\n\tat org.apache.zeppelin.scheduler.AbstractScheduler.runJob(AbstractScheduler.java:132)\n\tat org.apache.zeppelin.scheduler.FIFOScheduler.lambda$runJobInScheduler$0(FIFOScheduler.java:42)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1641362406341_1540538845",
      "id": "paragraph_1641362406341_1540538845",
      "dateCreated": "2022-01-05 06:00:06.341",
      "dateStarted": "2022-12-11 13:01:42.490",
      "dateFinished": "2022-12-11 13:01:42.505",
      "status": "ERROR"
    },
    {
      "text": "%pyspark\n\nfrom pyspark.rdd import portable_hash\nfrom random import random\nimport hashlib\n\nkeys \u003d [\"A\", \"B\", \"C\", \"D\"]\n\ndef hash_partitioning(key):\n    return portable_hash(key) % len(keys);\n    \n    \ndef hash_partitioning(key):\n    return hash(key) % len(keys)\n\n\ndef key_partitioning(key):\n    return keys.index(key)\n\n\nhashValue \u003d lambda str: portable_hash(str)\n\n\n#\n#\n#\ndef print_partitions(rdd):\n    numPartitions \u003d rdd.getNumPartitions()\n    print(\"Total partitions: {}\".format(numPartitions))\n    print(\"Partitioner: {}\".format(rdd.partitioner))\n    parts \u003d rdd.glom().collect()\n    i \u003d 0\n    j \u003d 0\n    for p in parts:\n        print(\"\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d Partition - {} \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\".format(i))\n        for r in p:\n            print(\"Row {} : {} : {}\".format(j, r, (hash(r[0])%numPartitions)))\n            j \u003d j+1\n        i \u003d i+1\n\n\n\nraw_values \u003d (\n    (\"A\", (\"Hindi\", 630, [10,15,18])),\n    (\"B\", (\"Hindi\", 720, [19,13,12])),\n    (\"C\", (\"Hindi\", 560, [19,13,12])),\n    (\"D\", (\"Hindi\", 870, [19,13,12])),\n    (\"A\", (\"English\", 710, [19,13,12])),\n    (\"B\", (\"English\", 730, [19,20,19])),\n    (\"C\", (\"English\", 620, [14,13,12])),\n    (\"D\", (\"English\", 800, [14,13,12])),\n    (\"A\", (\"Math\", 660, [17,18,18])),\n    (\"B\", (\"Math\", 750, [10,9,12])),\n    (\"C\", (\"Math\", 600, [10,12,13])),\n    (\"D\", (\"Math\", 840, [11,15,16])),\n    (\"A\", (\"Science\", 830, [17,18,18])),\n    (\"B\", (\"Science\", 700, [10,9,12])),\n    (\"C\", (\"Science\", 500, [10,12,13])),\n    (\"D\", (\"Science\", 880, [11,15,16]))\n)\n\nrawRDD \u003d spark.sparkContext.parallelize(raw_values).partitionBy(4, key_partitioning)\n\n# print(\"RDD-1 Partition Count : %i \" % (rawRDD.getNumPartitions()))\n# print(\"Values in RDD-1 : {0} \".format(rawRDD.glom().collect()))\nprint_partitions(rawRDD)",
      "user": "anonymous",
      "dateUpdated": "2022-06-29 02:44:46.283",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "fontSize": 12.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "Total partitions: 4\nPartitioner: \u003cpyspark.rdd.Partitioner object at 0x7f5a0f0260a0\u003e\n\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d Partition - 0 \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\nRow 0 : (\u0027A\u0027, (\u0027Hindi\u0027, 630, [10, 15, 18])) : 1\nRow 1 : (\u0027A\u0027, (\u0027English\u0027, 710, [19, 13, 12])) : 1\nRow 2 : (\u0027A\u0027, (\u0027Math\u0027, 660, [17, 18, 18])) : 1\nRow 3 : (\u0027A\u0027, (\u0027Science\u0027, 830, [17, 18, 18])) : 1\n\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d Partition - 1 \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\nRow 4 : (\u0027B\u0027, (\u0027Hindi\u0027, 720, [19, 13, 12])) : 1\nRow 5 : (\u0027B\u0027, (\u0027English\u0027, 730, [19, 20, 19])) : 1\nRow 6 : (\u0027B\u0027, (\u0027Math\u0027, 750, [10, 9, 12])) : 1\nRow 7 : (\u0027B\u0027, (\u0027Science\u0027, 700, [10, 9, 12])) : 1\n\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d Partition - 2 \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\nRow 8 : (\u0027C\u0027, (\u0027Hindi\u0027, 560, [19, 13, 12])) : 2\nRow 9 : (\u0027C\u0027, (\u0027English\u0027, 620, [14, 13, 12])) : 2\nRow 10 : (\u0027C\u0027, (\u0027Math\u0027, 600, [10, 12, 13])) : 2\nRow 11 : (\u0027C\u0027, (\u0027Science\u0027, 500, [10, 12, 13])) : 2\n\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d Partition - 3 \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\nRow 12 : (\u0027D\u0027, (\u0027Hindi\u0027, 870, [19, 13, 12])) : 3\nRow 13 : (\u0027D\u0027, (\u0027English\u0027, 800, [14, 13, 12])) : 3\nRow 14 : (\u0027D\u0027, (\u0027Math\u0027, 840, [11, 15, 16])) : 3\nRow 15 : (\u0027D\u0027, (\u0027Science\u0027, 880, [11, 15, 16])) : 3\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://nodemanager:35403/jobs/job?id\u003d0"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1641362321746_967238287",
      "id": "paragraph_1641362321746_967238287",
      "dateCreated": "2022-01-05 05:58:41.746",
      "dateStarted": "2022-06-24 03:13:07.700",
      "dateFinished": "2022-06-24 03:14:18.232",
      "status": "FINISHED"
    },
    {
      "title": "sortByKey",
      "text": "%pyspark\n\n#\nsortedRDD \u003d rawRDD.sortByKey(False,4)\nprint(\"Sorted By Key RDD\")\nprint_partitions(sortedRDD)\n\n#print(\"Sorted By Key RDD --\u003e : {0} \".format(sortedRDD.glom().collect()))",
      "user": "anonymous",
      "dateUpdated": "2022-06-20 03:19:53.836",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "fontSize": 12.0,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "Sorted By Key RDD\nTotal partitions: 4\nPartitioner: \u003cpyspark.rdd.Partitioner object at 0x7fb9b7dc23d0\u003e\n\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d Partition - 0 \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d Partition - 1 \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\nRow 0 : (\u0027D\u0027, (\u0027Hindi\u0027, 870, [19, 13, 12])) : 3\nRow 1 : (\u0027D\u0027, (\u0027English\u0027, 800, [14, 13, 12])) : 3\nRow 2 : (\u0027D\u0027, (\u0027Math\u0027, 840, [11, 15, 16])) : 3\nRow 3 : (\u0027D\u0027, (\u0027Science\u0027, 880, [11, 15, 16])) : 3\n\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d Partition - 2 \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\nRow 4 : (\u0027C\u0027, (\u0027Hindi\u0027, 560, [19, 13, 12])) : 2\nRow 5 : (\u0027C\u0027, (\u0027English\u0027, 620, [14, 13, 12])) : 2\nRow 6 : (\u0027C\u0027, (\u0027Math\u0027, 600, [10, 12, 13])) : 2\nRow 7 : (\u0027C\u0027, (\u0027Science\u0027, 500, [10, 12, 13])) : 2\n\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d Partition - 3 \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\nRow 8 : (\u0027B\u0027, (\u0027Hindi\u0027, 720, [19, 13, 12])) : 1\nRow 9 : (\u0027B\u0027, (\u0027English\u0027, 730, [19, 20, 19])) : 1\nRow 10 : (\u0027B\u0027, (\u0027Math\u0027, 750, [10, 9, 12])) : 1\nRow 11 : (\u0027B\u0027, (\u0027Science\u0027, 700, [10, 9, 12])) : 1\nRow 12 : (\u0027A\u0027, (\u0027Hindi\u0027, 630, [10, 15, 18])) : 1\nRow 13 : (\u0027A\u0027, (\u0027English\u0027, 710, [19, 13, 12])) : 1\nRow 14 : (\u0027A\u0027, (\u0027Math\u0027, 660, [17, 18, 18])) : 1\nRow 15 : (\u0027A\u0027, (\u0027Science\u0027, 830, [17, 18, 18])) : 1\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://nodemanager:46077/jobs/job?id\u003d37"
            },
            {
              "jobUrl": "http://nodemanager:46077/jobs/job?id\u003d38"
            },
            {
              "jobUrl": "http://nodemanager:46077/jobs/job?id\u003d39"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1641362383337_137132437",
      "id": "paragraph_1641362383337_137132437",
      "dateCreated": "2022-01-05 05:59:43.337",
      "dateStarted": "2022-06-20 03:19:53.869",
      "dateFinished": "2022-06-20 03:19:54.697",
      "status": "FINISHED"
    },
    {
      "title": "reduceByKey",
      "text": "%pyspark\n\n\"\"\"\nreduceByKey(func, [numPartitions])\n\nWhen called on a dataset of (K, V) pairs, returns a dataset of (K, V) pairs where the values for each key are aggregated \nusing the given reduce function func, which must be of type (V,V) \u003d\u003e V. Like in groupByKey, the number of reduce tasks is \nconfigurable through an optional second argument.\n\nOn applying reduceByKey on a dataset (K, V), before shuffeling of data the pairs on the same machine with the same key are combined.\n\n\"\"\"\n\n# (\"A\", (\"Hindi\", 630, [10,15,18])),  ---\u003e [(A, 630),(A, 630),(A, 630) .....] --\u003e [(A, 2830),(B, 2830)]\n#  \n\nreduceByKeyRDD \u003d rawRDD.map(lambda x : (x[0], x[1][1])).reduceByKey(lambda a, b: a + b)\n\nprint_partitions(reduceByKeyRDD)\n#print(\"Values in Y : {0} \".format(reduceByKeyRDD.collect()))\n",
      "user": "anonymous",
      "dateUpdated": "2022-06-24 03:37:23.688",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "fontSize": 12.0,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "Total partitions: 4\nPartitioner: \u003cpyspark.rdd.Partitioner object at 0x7f5a0f00e7f0\u003e\n\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d Partition - 0 \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d Partition - 1 \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\nRow 0 : (\u0027A\u0027, 2830) : 1\nRow 1 : (\u0027B\u0027, 2900) : 1\n\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d Partition - 2 \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\nRow 2 : (\u0027C\u0027, 2280) : 2\n\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d Partition - 3 \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\nRow 3 : (\u0027D\u0027, 3390) : 3\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://nodemanager:35403/jobs/job?id\u003d1"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1641362486699_2022535332",
      "id": "paragraph_1641362486699_2022535332",
      "dateCreated": "2022-01-05 06:01:26.699",
      "dateStarted": "2022-06-24 03:18:28.278",
      "dateFinished": "2022-06-24 03:18:28.786",
      "status": "FINISHED"
    },
    {
      "title": "aggregateByKey",
      "text": "%pyspark\n\n\"\"\"\naggregateByKey(zeroValue)(seqOp, combOp, [numPartitions])\n\naggregateByKey(self, zeroValue, seqFunc, combFunc, numPartitions\u003dNone, partitionFunc\u003d\u003cfunction portable_hash\u003e)\n\nWhen called on a dataset of (K, V) pairs, returns a dataset of (K, U) pairs where the values \nfor each key are aggregated using the given combine functions and a neutral \"zero\" value. \nAllows an aggregated value type that is different than the input value type, while avoiding \nunnecessary allocations. Like in groupByKey, the number of reduce tasks is configurable through \nan optional second argument.\n\n\"\"\"\n\n# Sequence operation : Finding Maximum Marks from a single partition\n# [U] \u003d\u003e [V]\n# Sequence operation for aggregation\n# acc\u003d0, subject\u003d(\"Hindi\", 630)\n# seqFunc compares the value with zeroValue and finds the greater one and will send it to combFunc.\n\ndef seqFunc(acc, value_part):\n    if(acc[1] \u003e value_part[1]):\n        return acc \n    else: \n        return value_part\n\n\n# Defining Sequential Operation and Combiner Operations\n# Combiner Operation : Finding Maximum Marks out Partition-Wise Accumulators\n# [U] op [U] \u003d\u003e [U]\n# Reduce/Combine operation for max aggregation#\ndef combFunc(acc1, acc2):\n    if(acc1[1] \u003e acc2[1]):\n        return acc1 \n    else:\n        return acc2\n \n# Let’s find the maximum marks and its subject for each student. First, we need to\n\"\"\"\n# (\"A\", (\"Hindi\", 630, [10,15,18])),  ---\u003e \n[\n    (\u0027A\u0027, (\u0027Hindi\u0027, 630)), \n    (\u0027A\u0027, (\u0027English\u0027, 710)), \n    (\u0027A\u0027, (\u0027Math\u0027, 660)), \n    (\u0027A\u0027, (\u0027Science\u0027, 830)), \n    (\u0027B\u0027, (\u0027Hindi\u0027, 720)), \n    (\u0027B\u0027, (\u0027English\u0027, 730)), \n    (\u0027B\u0027, (\u0027Math\u0027, 750)), \n    (\u0027B\u0027, (\u0027Science\u0027, 700)), \n    (\u0027C\u0027, (\u0027Hindi\u0027, 560)), \n    (\u0027C\u0027, (\u0027English\u0027, 620)), \n    (\u0027C\u0027, (\u0027Math\u0027, 600)), \n    (\u0027C\u0027, (\u0027Science\u0027, 500)), \n    (\u0027D\u0027, (\u0027Hindi\u0027, 870)), \n    (\u0027D\u0027, (\u0027English\u0027, 800)), \n    (\u0027D\u0027, (\u0027Math\u0027, 840)), \n    (\u0027D\u0027, (\u0027Science\u0027, 880))\n] \n\"\"\"\nstudentSubjectRDD \u003d rawRDD.map(lambda x : (x[0], (x[1][0], x[1][1])))\nprint(\"Values in studentSubjectRDD \")\nprint_partitions(studentSubjectRDD)\n\nstudentSubjectMaxRDD \u003d studentSubjectRDD.aggregateByKey((\"\", 0.0), seqFunc, combFunc)\nprint(\"Values in studentSubjectMaxRDD \")\nprint_partitions(studentSubjectMaxRDD)\n\n# print(\"Values in studentSubjectMaxRDD : {0} \".format(studentSubjectMaxRDD.collect()))\n\n",
      "user": "anonymous",
      "dateUpdated": "2022-06-24 05:11:47.161",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "fontSize": 12.0,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "Values in studentSubjectRDD \nTotal partitions: 4\nPartitioner: None\n\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d Partition - 0 \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\nRow 0 : (\u0027A\u0027, (\u0027Hindi\u0027, 630)) : 1\nRow 1 : (\u0027A\u0027, (\u0027English\u0027, 710)) : 1\nRow 2 : (\u0027A\u0027, (\u0027Math\u0027, 660)) : 1\nRow 3 : (\u0027A\u0027, (\u0027Science\u0027, 830)) : 1\n\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d Partition - 1 \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\nRow 4 : (\u0027B\u0027, (\u0027Hindi\u0027, 720)) : 1\nRow 5 : (\u0027B\u0027, (\u0027English\u0027, 730)) : 1\nRow 6 : (\u0027B\u0027, (\u0027Math\u0027, 750)) : 1\nRow 7 : (\u0027B\u0027, (\u0027Science\u0027, 700)) : 1\n\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d Partition - 2 \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\nRow 8 : (\u0027C\u0027, (\u0027Hindi\u0027, 560)) : 2\nRow 9 : (\u0027C\u0027, (\u0027English\u0027, 620)) : 2\nRow 10 : (\u0027C\u0027, (\u0027Math\u0027, 600)) : 2\nRow 11 : (\u0027C\u0027, (\u0027Science\u0027, 500)) : 2\n\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d Partition - 3 \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\nRow 12 : (\u0027D\u0027, (\u0027Hindi\u0027, 870)) : 3\nRow 13 : (\u0027D\u0027, (\u0027English\u0027, 800)) : 3\nRow 14 : (\u0027D\u0027, (\u0027Math\u0027, 840)) : 3\nRow 15 : (\u0027D\u0027, (\u0027Science\u0027, 880)) : 3\nTotal partitions: 4\nPartitioner: \u003cpyspark.rdd.Partitioner object at 0x7f5a0f3f3730\u003e\n\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d Partition - 0 \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d Partition - 1 \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\nRow 0 : (\u0027A\u0027, (\u0027Science\u0027, 830)) : 1\nRow 1 : (\u0027B\u0027, (\u0027Math\u0027, 750)) : 1\n\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d Partition - 2 \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\nRow 2 : (\u0027C\u0027, (\u0027English\u0027, 620)) : 2\n\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d Partition - 3 \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\nRow 3 : (\u0027D\u0027, (\u0027Science\u0027, 880)) : 3\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://nodemanager:35403/jobs/job?id\u003d35"
            },
            {
              "jobUrl": "http://nodemanager:35403/jobs/job?id\u003d36"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1656042314327_1681601132",
      "id": "paragraph_1656042314327_1681601132",
      "dateCreated": "2022-06-24 03:45:14.328",
      "dateStarted": "2022-06-24 04:54:04.037",
      "dateFinished": "2022-06-24 04:54:04.573",
      "status": "FINISHED"
    },
    {
      "title": "groupByKey",
      "text": "%pyspark\n\n\n\"\"\"\n\nWhen called on a dataset of (K, V) pairs, returns a dataset of (K, Iterable\u003cV\u003e) pairs.\n\nNote: If you are grouping in order to perform an aggregation (such as a sum or average) over each key, using reduceByKey or aggregateByKey will yield much better performance.\n\nNote: By default, the level of parallelism in the output depends on the number of partitions of the parent RDD. You can pass an optional numPartitions argument to set a different number of tasks.\n\n\nOn applying groupByKey() on a dataset of (K, V) pairs, the data shuffle according to the key value K in another RDD. In this transformation, lots of unnecessary data transfer over the network.\n\n\"\"\"\n\ndef group_by_f(element):\n    key \u003d element[0]\n    total \u003d sum(element[1])\n    return  (key, total)    \n        \n#rdd_3 \u003drdd_1.groupByKey().map(lambda e : (e[0], sum(e[1])))\n#print(\"Values in RDD-3 : {0} \".format(rdd_3.collect()))\n\ngroupByKeyRDD \u003drawRDD.map(lambda x : (x[0], x[1][1])).groupByKey()\n\nprocessedData \u003d groupByKeyRDD.map(group_by_f)\nprint_partitions(processedData)\n\n# print(\"Values in groupByKeyRDD : {0} \".format(processedData.collect()))\n\n\n",
      "user": "anonymous",
      "dateUpdated": "2022-06-24 03:32:11.942",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "fontSize": 12.0,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "Total partitions: 4\nPartitioner: None\n\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d Partition - 0 \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d Partition - 1 \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\nRow 0 : (\u0027A\u0027, 2830) : 1\nRow 1 : (\u0027B\u0027, 2900) : 1\n\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d Partition - 2 \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\nRow 2 : (\u0027C\u0027, 2280) : 2\n\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d Partition - 3 \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\nRow 3 : (\u0027D\u0027, 3390) : 3\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://nodemanager:35403/jobs/job?id\u003d4"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1641362643907_372254663",
      "id": "paragraph_1641362643907_372254663",
      "dateCreated": "2022-01-05 06:04:03.907",
      "dateStarted": "2022-06-24 03:32:11.959",
      "dateFinished": "2022-06-24 03:32:12.487",
      "status": "FINISHED"
    },
    {
      "title": "combineByKey",
      "text": " %pyspark\n\n# 1. createCombiner, which turns a V into a C (e.g., creates a one-element list)  V --\u003e C\n# 2. mergeValue, to merge a V into a C (e.g., adds it to the end of a list) C, V --\u003e C\n# 3. mergeCombiners, to combine two C’s into a single one. C , C --\u003e C\n\n#\n# create this “combiner” to be a tuple in the form of (sum, count). The very first step in this aggregation is then (value, 1), \n# where value is the first RDD value that combineByKey comes across and 1 initializes the count.\n\n# V --\u003e C\ndef createCombiner(v):\n    _c \u003d {\"total\":0, \"count\":0, \"min\":0, \"max\":0}\n    _c[\"total\"] \u003d v[2]\n    _c[\"count\"] \u003d 1\n    _c[\"min\"]   \u003d v[2]\n    _c[\"max\"]   \u003d v[2]\n    return _c\n#\n#\n#\n\n# C,V ---\u003e C\ndef merge(c, v):\n    \n    c[\"total\"] \u003d c[\"total\"] + v[2]\n    c[\"count\"] \u003d c[\"count\"] + 1\n    c[\"min\"]   \u003d min(c[\"min\"], v[2])\n    c[\"max\"]   \u003d max(c[\"max\"], v[2])\n    return c\n\n#\n#\n#\n\n# C,C ---\u003e C\ndef mergeCombiners(c1, c2):\n    c3 \u003d {}\n    c3[\"total\"] \u003d c1[\"total\"] + c2[\"total\"]\n    c3[\"count\"] \u003d c1[\"count\"] + c2[\"count\"]\n    c3[\"min\"]   \u003d min(c1[\"min\"], c2[\"min\"])\n    c3[\"max\"]   \u003d max(c1[\"max\"], c2[\"max\"])\n    return c3\n    \n\nstudent_marks \u003d [\n    (\"Joseph\", \"Maths\", 83),\n    (\"Joseph\", \"Physics\", 74),\n    (\"Joseph\", \"Chemistry\", 91),\n    (\"Joseph\", \"Biology\", 82),\n    (\"Jimmy\", \"Maths\", 69),\n    (\"Jimmy\", \"Physics\", 62),\n    (\"Jimmy\", \"Chemistry\", 97),\n    (\"Jimmy\", \"Biology\", 80),\n    (\"Tina\", \"Maths\", 78),\n    (\"Tina\", \"Physics\", 73),\n    (\"Tina\", \"Chemistry\", 68),\n    (\"Tina\", \"Biology\", 87),\n    (\"Thomas\", \"Maths\", 87),\n    (\"Thomas\", \"Physics\", 93),\n    (\"Thomas\", \"Chemistry\", 91),\n    (\"Thomas\", \"Biology\", 74),\n    (\"Cory\", \"Maths\", 56),\n    (\"Cory\", \"Physics\", 65),\n    (\"Cory\", \"Chemistry\", 71),\n    (\"Cory\", \"Biology\", 68),\n    (\"Jackeline\", \"Maths\", 86),\n    (\"Jackeline\", \"Physics\", 62),\n    (\"Jackeline\", \"Chemistry\", 75),\n    (\"Jackeline\", \"Biology\", 83),\n    (\"Juan\", \"Maths\", 63),\n    (\"Juan\", \"Physics\", 69),\n    (\"Juan\", \"Chemistry\", 64),\n    (\"Juan\", \"Biology\", 60)\n]\n\nstudent_marks_rdd \u003d spark.sparkContext.parallelize(student_marks).map(lambda x: (x[0],x))\n#for x in student_marks_rdd.collect():\n#   print(x)\n\nresult_rdd \u003d student_marks_rdd.combineByKey(createCombiner, merge, mergeCombiners )\nfor x in result_rdd.collect():\n  print(x)\n    \n#\n# Compute the Average\n#\nprint(\"\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003dCalulate Average\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\")\ndef cal_avg(v):\n    v[1][\"avg\"] \u003d v[1][\"total\"]/v[1][\"count\"]\n    return v\n    \navg_rdd \u003d result_rdd.map(cal_avg)\nfor x in avg_rdd.collect():\n  print(x)\n",
      "user": "anonymous",
      "dateUpdated": "2022-12-11 13:37:11.386",
      "progress": 0,
      "config": {
        "colWidth": 12.0,
        "fontSize": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/python",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "(\u0027Jimmy\u0027, {\u0027total\u0027: 308, \u0027count\u0027: 4, \u0027min\u0027: 62, \u0027max\u0027: 97})\n(\u0027Thomas\u0027, {\u0027total\u0027: 345, \u0027count\u0027: 4, \u0027min\u0027: 74, \u0027max\u0027: 93})\n(\u0027Juan\u0027, {\u0027total\u0027: 256, \u0027count\u0027: 4, \u0027min\u0027: 60, \u0027max\u0027: 69})\n(\u0027Tina\u0027, {\u0027total\u0027: 306, \u0027count\u0027: 4, \u0027min\u0027: 68, \u0027max\u0027: 87})\n(\u0027Joseph\u0027, {\u0027total\u0027: 330, \u0027count\u0027: 4, \u0027min\u0027: 74, \u0027max\u0027: 91})\n(\u0027Cory\u0027, {\u0027total\u0027: 260, \u0027count\u0027: 4, \u0027min\u0027: 56, \u0027max\u0027: 71})\n(\u0027Jackeline\u0027, {\u0027total\u0027: 306, \u0027count\u0027: 4, \u0027min\u0027: 62, \u0027max\u0027: 86})\n\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003dCalulate Average\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n(\u0027Jimmy\u0027, {\u0027total\u0027: 308, \u0027count\u0027: 4, \u0027min\u0027: 62, \u0027max\u0027: 97, \u0027avg\u0027: 77.0})\n(\u0027Thomas\u0027, {\u0027total\u0027: 345, \u0027count\u0027: 4, \u0027min\u0027: 74, \u0027max\u0027: 93, \u0027avg\u0027: 86.25})\n(\u0027Juan\u0027, {\u0027total\u0027: 256, \u0027count\u0027: 4, \u0027min\u0027: 60, \u0027max\u0027: 69, \u0027avg\u0027: 64.0})\n(\u0027Tina\u0027, {\u0027total\u0027: 306, \u0027count\u0027: 4, \u0027min\u0027: 68, \u0027max\u0027: 87, \u0027avg\u0027: 76.5})\n(\u0027Joseph\u0027, {\u0027total\u0027: 330, \u0027count\u0027: 4, \u0027min\u0027: 74, \u0027max\u0027: 91, \u0027avg\u0027: 82.5})\n(\u0027Cory\u0027, {\u0027total\u0027: 260, \u0027count\u0027: 4, \u0027min\u0027: 56, \u0027max\u0027: 71, \u0027avg\u0027: 65.0})\n(\u0027Jackeline\u0027, {\u0027total\u0027: 306, \u0027count\u0027: 4, \u0027min\u0027: 62, \u0027max\u0027: 86, \u0027avg\u0027: 76.5})\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://resourcemanager:8088/proxy/application_1670734498586_0001//jobs/job?id\u003d8"
            },
            {
              "jobUrl": "http://resourcemanager:8088/proxy/application_1670734498586_0001//jobs/job?id\u003d9"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1641381962217_955178022",
      "id": "paragraph_1641381962217_955178022",
      "dateCreated": "2022-01-05 11:26:02.217",
      "dateStarted": "2022-12-11 13:37:11.391",
      "dateFinished": "2022-12-11 13:37:11.756",
      "status": "FINISHED"
    },
    {
      "title": "mapPartitions",
      "text": "%md\n\nmapPartitions is a transformation operation model of PySpark RDD.\nmapPartitions is applied over RDD in PySpark so that the Data frame needs to be converted to RDD.\nmapPartitions are applied over the logic or functions that are heavy transformations.\nmapPartitions is applied to a specific partition in the model rather than each and every row model in PySpark.\nmapPartitions keep the result in the partition memory.\nmapPartitions is a faster and cheap data processing model.\n",
      "user": "anonymous",
      "dateUpdated": "2022-06-19 08:07:12.928",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "fontSize": 12.0,
        "editorHide": true,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003emapPartitions is a transformation operation model of PySpark RDD.\u003cbr /\u003e\nmapPartitions is applied over RDD in PySpark so that the Data frame needs to be converted to RDD.\u003cbr /\u003e\nmapPartitions are applied over the logic or functions that are heavy transformations.\u003cbr /\u003e\nmapPartitions is applied to a specific partition in the model rather than each and every row model in PySpark.\u003cbr /\u003e\nmapPartitions keep the result in the partition memory.\u003cbr /\u003e\nmapPartitions is a faster and cheap data processing model.\u003c/p\u003e\n\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1655626003295_2029296457",
      "id": "paragraph_1655626003295_2029296457",
      "dateCreated": "2022-06-19 08:06:43.295",
      "dateStarted": "2022-06-19 08:07:12.928",
      "dateFinished": "2022-06-19 08:07:12.938",
      "status": "FINISHED"
    },
    {
      "title": "mapPartitions",
      "text": "%pyspark\n\n\"\"\"\nmapPartitions(func) : \nSimilar to map, but runs separately on each partition (block) of the RDD, so func must be of type Iterator\u003cT\u003e \u003d\u003e Iterator\u003cU\u003e when running on an RDD of type T.\n\n\"\"\"\n\n#\n#\n#\ndef mapFunction(iterator):\n    #perform heavy initializations like Databse connections\n    total \u003d 0\n    for e in iterator:\n        # perform operations for element in a partition\n        key \u003d e[0]\n        percentage \u003d  e[1][1]/10\n        yield (e[0], percentage)\n\nmapPartitionsRDD \u003d rawRDD.mapPartitions(mapFunction, preservesPartitioning\u003dTrue)\n\nprint_partitions(mapPartitionsRDD)\n\n# print(mapPartitionsRDD.glom().collect())\n",
      "user": "anonymous",
      "dateUpdated": "2022-06-24 03:41:59.213",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "fontSize": 12.0,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "Total partitions: 4\nPartitioner: \u003cpyspark.rdd.Partitioner object at 0x7f5a0f0260a0\u003e\n\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d Partition - 0 \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\nRow 0 : (\u0027A\u0027, 63.0) : 1\nRow 1 : (\u0027A\u0027, 71.0) : 1\nRow 2 : (\u0027A\u0027, 66.0) : 1\nRow 3 : (\u0027A\u0027, 83.0) : 1\n\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d Partition - 1 \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\nRow 4 : (\u0027B\u0027, 72.0) : 1\nRow 5 : (\u0027B\u0027, 73.0) : 1\nRow 6 : (\u0027B\u0027, 75.0) : 1\nRow 7 : (\u0027B\u0027, 70.0) : 1\n\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d Partition - 2 \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\nRow 8 : (\u0027C\u0027, 56.0) : 2\nRow 9 : (\u0027C\u0027, 62.0) : 2\nRow 10 : (\u0027C\u0027, 60.0) : 2\nRow 11 : (\u0027C\u0027, 50.0) : 2\n\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d Partition - 3 \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\nRow 12 : (\u0027D\u0027, 87.0) : 3\nRow 13 : (\u0027D\u0027, 80.0) : 3\nRow 14 : (\u0027D\u0027, 84.0) : 3\nRow 15 : (\u0027D\u0027, 88.0) : 3\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://nodemanager:35403/jobs/job?id\u003d5"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1641362950471_439046731",
      "id": "paragraph_1641362950471_439046731",
      "dateCreated": "2022-01-05 06:09:10.471",
      "dateStarted": "2022-06-24 03:41:59.230",
      "dateFinished": "2022-06-24 03:41:59.445",
      "status": "FINISHED"
    },
    {
      "text": "%pyspark\n\n#\n# Find min \u0026 max marks in each partition.\n#\ndef minmax(iterator):\n    firsttime\u003d0\n    min \u003d None\n    max \u003d None\n    for e in iterator :\n        x \u003d e[1][1]\n        if firsttime \u003d\u003d 0:\n            min \u003d x\n            max \u003d x\n            firsttime+\u003d1\n        else:\n            if x \u003e max :\n                max \u003d x\n            if x \u003c min :\n                min \u003d x\n    #\n    yield (min, max)\n     \n     \nprint(\"\u003d\u003d\u003d\u003d Min Max in each Partition \u003d\u003d\u003d\u003d \")\nminMaxRDD \u003d rawRDD.mapPartitions(minmax, preservesPartitioning\u003dTrue)\nprint(minMaxRDD.glom().collect())   ",
      "user": "anonymous",
      "dateUpdated": "2022-06-22 05:25:26.988",
      "progress": 5,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "fontSize": 12.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u003d\u003d\u003d\u003d Min Max in each Partition \u003d\u003d\u003d\u003d \n[[(630, 830)], [(700, 750)], [(500, 620)], [(800, 880)]]\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://zeppelin:4040/jobs/job?id\u003d4"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1655632683493_755124324",
      "id": "paragraph_1655632683493_755124324",
      "dateCreated": "2022-06-19 09:58:03.493",
      "dateStarted": "2022-06-22 05:25:27.027",
      "dateFinished": "2022-06-22 05:25:27.737",
      "status": "FINISHED"
    },
    {
      "title": "mapPartitionsWithIndex",
      "text": "%pyspark\n\n\"\"\"\n\nmapPartitionsWithIndex(func)\n\nSimilar to mapPartitions, but also provides func with an integer value representing the index of the partition, \nso func must be of type (Int, Iterator\u003cT\u003e) \u003d\u003e Iterator\u003cU\u003e when running on an RDD of type T.\n\n\n\"\"\"\ndef mpwi_function(index, iterator):\n    total \u003d 0;\n    for e in iterator:\n        key \u003d e[0]\n        value \u003d sum(e[1][2])\n        total \u003d total + value\n        \n    yield str(index) + \"-\" + key\n    yield total\n    \n    \nmapPartitionsWithIndexRDD \u003d rawRDD.mapPartitionsWithIndex(mpwi_function)\n\nprint(mapPartitionsWithIndexRDD.glom().collect())\n",
      "user": "anonymous",
      "dateUpdated": "2022-06-24 03:43:44.966",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "fontSize": 12.0,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "[[\u00270-D\u0027, 175], [\u00271-D\u0027, 180], [\u00272-D\u0027, 161], [\u00273-D\u0027, 161]]\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://nodemanager:34007/jobs/job?id\u003d20"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1641372418815_1878083143",
      "id": "paragraph_1641372418815_1878083143",
      "dateCreated": "2022-01-05 08:46:58.815",
      "dateStarted": "2022-06-19 10:15:23.445",
      "dateFinished": "2022-06-19 10:15:23.619",
      "status": "FINISHED"
    }
  ],
  "name": "pyspark-paired-rdd-transformations",
  "id": "2GSKPE7D5",
  "defaultInterpreterGroup": "spark",
  "version": "0.10.0",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {},
  "config": {
    "isZeppelinNotebookCronEnable": false
  },
  "info": {}
}